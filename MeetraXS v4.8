#!/usr/bin/env python3
"""
meetraxs_enhanced_agi.py
MeetraXS (Enhanced AGI Edition) - single-file research scaffold with Ollama local LLM integration
Enhanced Version (October 2025 Update) - Global Revolution Edition v4.7-20251018-agi_step3-fixed-enhanced with v2 Memory and Improvements:
- Integrated advanced hierarchical memory: ShortTerm, Episodic (FAISS+CLIP), Semantic (SQLite), Self, Procedural.
- RAG: Auto-injects top-5 memories into prompts.
- Consolidation: Background thread for sleep cycles with adaptive interval.
- Dream: Periodic self-reflection on episodes.
- Cross-modal: Image/text linked via CLIP.
- Decay: Time/importance-based forgetting.
- Persistence: Disk-based, low-RAM (paging with overflow handling).
- FIXED/ENHANCED: All previous fixes + new improvements: FAISS trained on real data, paging for overflow, adaptive consolidation,
  enhanced streaming in Ollama, shell command whitelist, CPU limit for safe_eval, dynamic dataset in SelfTrainer,
  experience archiving in AdaptationLoop, pagination in GUI memory, non-blocking Matplotlib, dependency check,
  signal handler for shutdown, structured logging with rotation. NEW: Fixed overflow retrieval, image handling, LoRA targets,
  benchmark scoring, GoalManager locks, Ollama JSON format, embed device.
"""
# Imports at top
import os
import time
import json
import threading
import tempfile
import traceback
import random
import queue
import signal
import sys
import logging
from collections import deque
from dataclasses import dataclass, asdict, field
from typing import List, Dict, Tuple, Any, Optional
import re
import math
import subprocess
import shlex
import psutil
import shutil
import sqlite3
try:
    import resource
except ImportError:
    import psutil, os
    class resource:
        RUSAGE_SELF = None
        @staticmethod
        def getrusage(_):
            return type('Usage', (), {'ru_maxrss': psutil.Process(os.getpid()).memory_info().rss / 1024})()
# ML / infra imports (optional)
try:
    import torch
    import open_clip
    from PIL import Image
    import numpy as np
    from sentence_transformers import SentenceTransformer
    import faiss
    import networkx as nx
    import sympy # For symbolic reasoning
except Exception as e:
    print(f"ML imports failed: {e}")
    torch = None
    open_clip = None
    Image = None
    np = None
    SentenceTransformer = None
    faiss = None
    nx = None
    sympy = None
# HTTP / scraping & utilities
import requests
from bs4 import BeautifulSoup
import urllib.parse
import ast
from dotenv import load_dotenv
load_dotenv()
# GUI enhancements
try:
    import customtkinter as ctk
    ctk.set_appearance_mode("Dark")
    ctk.set_default_color_theme("blue")
    from tkinter import messagebox
    from matplotlib.backends.backend_tkagg import FigureCanvasTkAgg
except Exception as e:
    print(f"GUI imports failed: {e}")
    ctk = None
    messagebox = None
    FigureCanvasTkAgg = None
# Hybrid frameworks - Enhanced with MetaGPT-like multi-agent and LangGraph
    try:
        try:
            from autogen.agentchat import AssistantAgent, UserProxyAgent, GroupChat, GroupChatManager
        except ImportError:
            print("⚠️  AutoGen not available — disabling multi-agent mode.")
            AssistantAgent = UserProxyAgent = GroupChat = GroupChatManager = None
    except ImportError:
        print("⚠️  AutoGen not available — disabling multi-agent mode.")
        AssistantAgent = UserProxyAgent = GroupChat = GroupChatManager = None
    from semantic_kernel import Kernel
    from langgraph.graph import StateGraph, MessagesState, START, END
    from langgraph.prebuilt import tools_condition, ToolNode
    from langchain_core.messages import HumanMessage, SystemMessage
except ImportError as e:
    print(f"Framework imports failed: {e}")
    AssistantAgent = None
    UserProxyAgent = None
    GroupChat = None
    GroupChatManager = None
    Kernel = None
    StateGraph = None
    MessagesState = None
    tools_condition = None
    ToolNode = None
    HumanMessage = None
    SystemMessage = None
# Compression/verification
import snappy
# For LoRA fine-tuning
try:
    from peft import LoraConfig, get_peft_model
    from transformers import AutoModelForCausalLM, AutoTokenizer, Trainer, TrainingArguments, BitsAndBytesConfig, AutoConfig
    from datasets import load_dataset
except ImportError as e:
    print(f"LoRA imports failed: {e}")
    LoraConfig = None
    get_peft_model = None
    AutoModelForCausalLM = None
    AutoTokenizer = None
    Trainer = None
    TrainingArguments = None
    load_dataset = None
    BitsAndBytesConfig = None
    AutoConfig = None
# For GUI visualization
try:
    import matplotlib.pyplot as plt
except ImportError:
    plt = None
# Config & Safety gates - Updated with ethical weighting
version = "4.7-20251018-agi_step3-fixed-enhanced"
# =================== CONFIGURATION ===================
CFG = {
    "version": version,
    "clip_model": os.getenv("MEETRAXS_CLIP_MODEL", "ViT-B-32"),
    "clip_pretrained": os.getenv("MEETRAXS_CLIP_PRETRAIN", "laion2b_s34b_b79k"),
    "memory_embed_model": os.getenv("MEETRAXS_EMBED_MODEL", "all-MiniLM-L6-v2"),
    "faiss_index_path": os.getenv("MEETRAXS_FAISS_INDEX", "meetraxs_faiss.index"),
    "memory_meta_path": os.getenv("MEETRAXS_META_PATH", "meetraxs_meta.json"),
    "skill_lib_path": os.getenv("MEETRAXS_SKILLS", "meetraxs_skills.json"),
    "goal_state_path": "meetraxs_goals.json",
    "self_model_path": "meetraxs_self_model.json",
    "adaptation_data_path": "adaptation_data.json",
    "max_tool_calls": 15,
    "max_reason_steps": 30,
    "python_exec_timeout": 10,
    "learning_default_sleep": 300,
    "allow_danger_env": "ALLOW_DANGEROUS_TOOLS",
    "self_update_token_env": "ALLOW_SELF_UPDATE_TOKEN",
    "enable_learning_env": "MEETRAXS_ENABLE_LEARNING",
    "emergency_stop_file": os.getenv("MEETRAXS_EMERGENCY_STOP_FILE", "/tmp/meetraxs_emergency_stop"),
    "max_anomalies": 5,
    "prompt_version": "4.5",
    "ethical_weight_threshold": 0.7,
    "ollama_retries": 3,
    "ollama_fail_threshold": 4,
    "bias_phrases": ["biased term1", "biased term2", "racist", "sexist", "hate speech"],
    "faiss_nlist": 64,
    "faiss_m": 8,
    "faiss_nbits": 8,
    "faiss_ntrain_factor": 156,
    "lora_rank": 16,
    "training_epochs": 1,
    "logo_path": "meetraxs_logo.png",
    "feedback_data_path": "feedback_data.json",
    "models": {
    # ✅ Primary and fallback — both use Qwen (light + reliable)
    "small": os.getenv("OLLAMA_MODEL_SMALL", "qwen2.5:1.5b-instruct-q4_0"),
    "general": os.getenv("OLLAMA_MODEL_GENERAL", "qwen2.5:1.5b-instruct-q4_0"),
    "math": os.getenv("OLLAMA_MODEL_MATH", "qwen2.5:1.5b-instruct-q4_0"),
    "efficient": os.getenv("OLLAMA_MODEL_EFFICIENT", "qwen2.5:1.5b-instruct-q4_0"),

    # ⚠️ Placeholder only — do not actually load large models on 8 GB
    "large": os.getenv("OLLAMA_MODEL_LARGE", "qwen2.5:1.5b-instruct-q4_0"),
},
    "adaptation_interval": 600,
    "adaptation_threshold": 0.6,
    "ollama_num_thread": os.cpu_count() or 4,
    "ollama_num_gpu": -1,
    "allowed_shell_commands": ["ls", "pwd", "echo", "cat", "head", "tail"],
    "max_dataset_size": 500,
    # ✅ Primary and local LLM
    "llm_model": os.getenv("MEETRAXS_LLM", "qwen2.5:1.5b-instruct-q4_0"),
    "local_llm_model": os.getenv("MEETRAXS_LOCAL_LLM", "qwen2.5:1.5b-instruct-q4_0"),
    # ✅ Device handling
    "device": "cuda" if (torch and torch.cuda.is_available()) else "cpu",
    # ✅ Safety system prompt
    "default_system_prompt": (
        f"Version {version}: You are MeetraXS, a collaborative open AGI prototype. "
        "Refuse illegal/harmful requests. Prioritize ethics, safety, and global impact."
    ),
    # ✅ Tool safety limits
    "dangerous_tools": {
        "RUN_SHELL_COMMAND", "SELF_UPDATE", "SELF_REPLICATE",
        "EDIT_SYSTEM_FILE", "DELETE_FILE", "DELETE_DIRECTORY",
        "KILL_PROCESS", "MANAGE_SERVICE",
    }
}
# =====================================================
# ✅ Environment settings
import os
os.environ["OLLAMA_NO_GPU"] = "1"
os.environ["OLLAMA_HOST"] = os.getenv("OLLAMA_HOST", "localhost")
os.environ["OLLAMA_PORT"] = os.getenv("OLLAMA_PORT", "11434")
OLLAMA_API_URL = os.getenv("OLLAMA_API_URL", "http://localhost:11434/api/generate")
OLLAMA_TIMEOUT = int(os.getenv("OLLAMA_TIMEOUT", "600"))
# Structured Logging Setup
logger = logging.getLogger("MeetraXS")
logger.setLevel(logging.INFO)
handler = logging.handlers.RotatingFileHandler("meetraxs.log", maxBytes=10*1024*1024, backupCount=5)
formatter = logging.Formatter("[%(asctime)s] [%(levelname)s] %(message)s")
handler.setFormatter(formatter)
logger.addHandler(handler)
handler = logging.StreamHandler()
handler.setFormatter(formatter)
logger.addHandler(handler)
def log(msg: str, level: str = "INFO", trace: bool = False):
    level_lower = level.lower().replace('warn', 'warning')
    level_func = getattr(logger, level_lower)
    level_func(msg)
    if trace:
        logger.debug(traceback.format_stack())
# Dependency Check
def check_dependencies():
    deps = {
        "torch": torch,
        "faiss": faiss,
        "customtkinter": ctk,
        "sentence_transformers": SentenceTransformer,
        "open_clip": open_clip,
        "networkx": nx,
        "sympy": sympy,
        "matplotlib": plt,
        "PIL": Image,
        "numpy": np,
    }
    missing = [name for name, dep in deps.items() if dep is None]
    if missing:
        log(f"Missing dependencies: {', '.join(missing)}. Some features disabled.", "WARN")
    return missing
check_dependencies()
# Signal Handler for Graceful Shutdown
def handle_shutdown(signum, frame):
    log("Received shutdown signal; initiating graceful shutdown.", "INFO")
    # Global components will be stopped in main()
    sys.exit(0)
signal.signal(signal.SIGTERM, handle_shutdown)
signal.signal(signal.SIGINT, handle_shutdown)
# --- meetraxs_memory_v2.py (Integrated) ---
@dataclass
class MemoryRecord:
    content: str
    embedding: Any = None
    metadata: Dict[str, Any] = None
    id: str = None
    def __post_init__(self):
        if self.metadata is None:
            self.metadata = {}
        if 'time' not in self.metadata:
            self.metadata['time'] = time.time()
        if 'importance' not in self.metadata:
            self.metadata['importance'] = 1.0
        if self.id is None:
            self.id = f"{self.metadata['time']:.0f}"
class ShortTermMemory:
    def __init__(self, max_size: int = 50):
        self.buffer: deque[Dict[str, Any]] = deque(maxlen=max_size)
        self.lock = threading.Lock()
    def add_interaction(self, user_input: str, agent_output: str, metadata: Dict[str, Any] = None):
        with self.lock:
            entry = {
                'user': user_input,
                'agent': agent_output,
                'time': time.time(),
                'metadata': metadata or {}
            }
            self.buffer.append(entry)
    def get_recent(self, n: int = 10) -> List[Dict[str, Any]]:
        with self.lock:
            return list(self.buffer)[-n:]
    def flush_to_episodic(self, episodic: 'EpisodicMemory'):
        with self.lock:
            for entry in self.buffer:
                meta = entry['metadata'].copy()
                meta['source'] = 'short_term'
                if meta.get('type') == 'image':
                    content = meta.get('image_path', f"User: {entry['user']} | Agent: {entry['agent']}")
                else:
                    content = f"User: {entry['user']} | Agent: {entry['agent']}"
                episodic.add_memory(MemoryRecord(content=content, metadata=meta))
            self.buffer.clear()
class EpisodicMemory:
    def __init__(self, index_path: str = "meetraxs_episodic.faiss", meta_path: str = "meetraxs_episodic_meta.json"):
        self.index_path = index_path
        self.meta_path = meta_path
        self.meta: List[MemoryRecord] = []
        self.index = None
        self.embed_model = None
        self.device = None
        if SentenceTransformer:
            self.embed_model = SentenceTransformer("all-MiniLM-L6-v2")
            self.device = CFG["device"]
            self.embed_model.to(self.device)
        self.clip_model = None
        self.clip_preprocess = None
        if open_clip and torch:
            self.clip_model, _, self.clip_preprocess = open_clip.create_model_and_transforms("ViT-B-32", pretrained="laion2b_s34b_b79k")
            self.clip_model.eval()
            self.device = "cuda" if torch.cuda.is_available() else "cpu"
            self.clip_model.to(self.device)
        self.lock = threading.Lock()
        self.overflow_path = None
        self._load_or_init()
    def _load_or_init(self):
        if faiss and os.path.exists(self.index_path) and os.path.exists(self.meta_path):
            self.index = faiss.read_index(self.index_path)
            with open(self.meta_path, 'r') as f:
                all_meta = json.load(f)
            self.meta = [MemoryRecord(
                content=r['content'],
                embedding=np.array(r['embedding']),
                metadata=r['metadata'],
                id=r['id']
            ) for r in all_meta[:100]]
            if len(all_meta) > 100:
                self.overflow_path = tempfile.NamedTemporaryFile(delete=False).name
                with open(self.overflow_path, 'w') as f:
                    json.dump(all_meta[100:], f)
        else:
            dim = 512
            quantizer = faiss.IndexFlatIP(dim)
            self.index = faiss.IndexIVFFlat(quantizer, dim, 100)
            self.meta = []
    def _train_faiss(self):
        if not self.meta or not faiss:
            return
        # Sample from overflow if exists for training
        train_embs = []
        train_embs.extend([m.embedding for m in self.meta if m.embedding is not None])
        if self.overflow_path and os.path.exists(self.overflow_path):
            with open(self.overflow_path, 'r') as f:
                overflow = json.load(f)
            sample_size = min(50, len(overflow))
            sampled = random.sample(overflow, sample_size)
            train_embs.extend([np.array(r['embedding']) for r in sampled])
        embeddings = np.array(train_embs)
        if len(embeddings) < CFG["faiss_ntrain_factor"]:
            return
        dim = embeddings.shape[1]
        quantizer = faiss.IndexFlatIP(dim)
        self.index = faiss.IndexIVFFlat(quantizer, dim, CFG["faiss_nlist"])
        self.index.train(embeddings[:CFG["faiss_ntrain_factor"]].astype('float32'))
    def _load_overflow(self) -> List[MemoryRecord]:
        if not self.overflow_path or not os.path.exists(self.overflow_path):
            return []
        with open(self.overflow_path, 'r') as f:
            overflow = json.load(f)
        return [MemoryRecord(
            content=r['content'],
            embedding=np.array(r['embedding']),
            metadata=r['metadata'],
            id=r['id']
        ) for r in overflow]
    def _save(self):
        with self.lock:
            all_meta = []
            for m in self.meta:
                d = asdict(m)
                d['embedding'] = m.embedding.tolist()
                all_meta.append(d)
            if self.overflow_path:
                with open(self.overflow_path, 'r') as f:
                    overflow_meta = json.load(f)
                all_meta.extend(overflow_meta)
            if self.index:
                faiss.write_index(self.index, self.index_path)
            with open(self.meta_path, 'w') as f:
                json.dump(all_meta, f)
    def add_memory(self, record: MemoryRecord):
        with self.lock:
            if record.metadata.get('type') == 'image' and Image and self.clip_model:
                img = Image.open(record.content)
                img_tensor = self.clip_preprocess(img).unsqueeze(0).to(self.device)
                with torch.no_grad():
                    emb = self.clip_model.encode_image(img_tensor).cpu().numpy().flatten()
            else:
                if self.clip_model:
                    tokens = open_clip.tokenize([record.content]).to(self.device)
                    with torch.no_grad():
                        features = self.clip_model.encode_text(tokens)
                    text_emb = features.detach().cpu().numpy().flatten()
                    emb = text_emb 
                elif self.embed_model:
                    emb = self.embed_model.encode([record.content])[0]
                else:
                    emb = np.random.random(512).astype('float32')
            record.embedding = emb.astype('float32')
            norm = np.linalg.norm(emb)
            if norm > 0:
                record.embedding /= norm
            if self.index:
                try:
                    # Ensure index is trained if it's an IVF index
                    is_ivf = hasattr(self.index, 'is_trained')
                    if is_ivf and not getattr(self.index, 'is_trained'):
                        # Attempt to gather embeddings for training (including this new one)
                        train_embs = [m.embedding for m in self.meta if getattr(m, 'embedding', None) is not None]
                        train_embs.append(record.embedding)
                        if faiss and len(train_embs) >= CFG.get('faiss_ntrain_factor', 156):
                            arr = np.array(train_embs).astype('float32')
                            try:
                                self.index.train(arr[:CFG.get('faiss_ntrain_factor')])
                                log('Trained FAISS IVF index with available embeddings.', 'INFO')
                            except Exception as e:
                                log(f'FAISS train failed: {e}. Falling back to IndexFlatIP.', 'WARN')
                                # fallback to flat index
                                dim = record.embedding.shape[0]
                                self.index = faiss.IndexFlatIP(dim)
                                # re-add existing embeddings
                                existing = [m.embedding for m in self.meta if getattr(m, 'embedding', None) is not None]
                                if existing:
                                    self.index.add(np.array(existing).astype('float32'))
                        else:
                            # Not enough embeddings to train an IVF — fall back to a flat index
                            if faiss:
                                dim = record.embedding.shape[0]
                                log('Not enough embeddings to train IVF index; switching to IndexFlatIP fallback.', 'INFO')
                                self.index = faiss.IndexFlatIP(dim)
                                existing = [m.embedding for m in self.meta if getattr(m, 'embedding', None) is not None]
                                if existing:
                                    self.index.add(np.array(existing).astype('float32'))
                    # Finally, add the new vector
                    try:
                        to_add = np.array([record.embedding]).astype('float32')
                        self.index.add(to_add)
                    except Exception as e:
                        log(f'Failed to add vector to FAISS index: {e}', 'ERROR')
                        # As a last resort, switch to a flat index and try again
                        if faiss:
                            dim = record.embedding.shape[0]
                            self.index = faiss.IndexFlatIP(dim)
                            all_embs = [m.embedding for m in self.meta if getattr(m, 'embedding', None) is not None]
                            all_embs.append(record.embedding)
                            self.index.add(np.array(all_embs).astype('float32'))
                except Exception as e:
                    log(f'Unexpected error handling FAISS index: {e}', 'ERROR')
            self.meta.append(record)
            if len(self.meta) > 100:
                self._apply_decay()
                if len(self.meta) > 100:
                    self._save_overflow()
    def _save_overflow(self):
        if self.overflow_path:
            os.unlink(self.overflow_path)
        self.overflow_path = tempfile.NamedTemporaryFile(delete=False).name
        overflow_list = self.meta[100:]
        self.meta = self.meta[:100]
        with open(self.overflow_path, 'w') as f:
            json.dump([asdict(m) for m in overflow_list], f)
    def _apply_decay(self):
        now = time.time()
        to_prune = []
        for i, rec in enumerate(self.meta):
            age_days = (now - rec.metadata['time']) / 86400
            score = rec.metadata['importance'] * math.exp(-age_days / 30)
            if score < 0.3:
                to_prune.append(i)
        for i in sorted(to_prune, reverse=True):
            del self.meta[i]
            if self.index:
                self.index.remove_ids(np.array([i]))
        if to_prune:
            self._save()
            self._train_faiss()
    def retrieve_relevant(self, query: str, top_k: int = 5, is_image_query: bool = False) -> List[MemoryRecord]:
        with self.lock:
            if not self.index or self.index.ntotal == 0:
                return []
            overflow_list = self._load_overflow()
            all_meta = self.meta + overflow_list
            if is_image_query and Image and self.clip_model:
                img = Image.open(query)
                q_emb = self.clip_model.encode_image(self.clip_preprocess(img).unsqueeze(0).to(self.device)).cpu().numpy().flatten()
            else:
                if self.clip_model:
                    q_emb = self.clip_model.encode_text(open_clip.tokenize([query]).to(self.device)).cpu().numpy().flatten()
                elif self.embed_model:
                    q_emb = self.embed_model.encode([query])[0]
                else:
                    q_emb = np.random.random(512).astype('float32')
            q_emb = q_emb.astype('float32')
            norm = np.linalg.norm(q_emb)
            if norm > 0:
                q_emb /= norm
            scores, indices = self.index.search(np.array([q_emb]), k=top_k)
            results = []
            for idx in indices[0]:
                if 0 <= idx < len(all_meta):
                    results.append(all_meta[idx])
            return results[:top_k]
class SemanticMemory:
    def __init__(self, db_path: str = "meetraxs_semantic.db"):
        self.db_path = db_path
        self.conn = sqlite3.connect(db_path, check_same_thread=False)
        self._init_schema()
    def _init_schema(self):
        cursor = self.conn.cursor()
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS concepts (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                summary TEXT NOT NULL,
                category TEXT,
                time REAL,
                importance REAL DEFAULT 1.0
            )
        """)
        cursor.execute("""
            CREATE TABLE IF NOT EXISTS relations (
                id INTEGER PRIMARY KEY AUTOINCREMENT,
                from_concept INTEGER,
                to_concept INTEGER,
                relation_type TEXT,
                FOREIGN KEY(from_concept) REFERENCES concepts(id),
                FOREIGN KEY(to_concept) REFERENCES concepts(id)
            )
        """)
        self.conn.commit()
    def add_concept(self, summary: str, category: str = 'general', importance: float = 1.0):
        cursor = self.conn.cursor()
        cursor.execute("INSERT INTO concepts (summary, category, time, importance) VALUES (?, ?, ?, ?)",
                       (summary, category, time.time(), importance))
        self.conn.commit()
        return cursor.lastrowid
    def add_relation(self, from_id: int, to_id: int, relation_type: str = 'related'):
        cursor = self.conn.cursor()
        cursor.execute("INSERT INTO relations (from_concept, to_concept, relation_type) VALUES (?, ?, ?)",
                       (from_id, to_id, relation_type))
        self.conn.commit()
    def query_concepts(self, keyword: str, limit: int = 5) -> List[Tuple[int, str]]:
        cursor = self.conn.cursor()
        cursor.execute("SELECT id, summary FROM concepts WHERE summary LIKE ? LIMIT ?", (f"%{keyword}%", limit))
        return cursor.fetchall()
class SelfMemory:
    def __init__(self, path: str = "meetraxs_self_reflections.json"):
        self.path = path
        self.reflections: List[Dict[str, Any]] = []
        self.lock = threading.Lock()
        self._load()
    def _load(self):
        if os.path.exists(self.path):
            with open(self.path, 'r') as f:
                self.reflections = json.load(f)
    def _save(self):
        with self.lock:
            with open(self.path, 'w') as f:
                json.dump(self.reflections, f, indent=2)
    def add_reflection(self, content: str, metadata: Dict[str, Any] = None):
        entry = {
            'content': content,
            'time': time.time(),
            'metadata': metadata or {}
        }
        with self.lock:
            self.reflections.append(entry)
            self._save()
    def dream(self, episodic: EpisodicMemory):
        recent = episodic.retrieve_relevant("self-reflection", top_k=100)
        if not recent:
            return "No recent memories to dream on."
        summary = f"Key learnings from {len(recent)} episodes: "
        for rec in recent[-10:]:
            summary += f"\n- {rec.content[:100]} (imp: {rec.metadata['importance']:.2f})"
        self.add_reflection(summary, {'type': 'dream'})
        with open("meetraxs_self_reflections.txt", 'a') as f:
            f.write(f"\n--- Dream at {time.strftime('%Y-%m-%d %H:%M:%S')} ---\n{summary}\n")
        return summary
class ProceduralMemory:
    def __init__(self, path: str = "meetraxs_procedural_skills.json"):
        self.path = path
        self.skills: Dict[str, Dict[str, Any]] = {}
        if os.path.exists(path):
            with open(path, 'r') as f:
                self.skills = json.load(f)
    def add_skill(self, name: str, pattern: str, success_rate: float = 1.0):
        self.skills[name] = {'pattern': pattern, 'success_rate': success_rate, 'uses': 0}
        with open(self.path, 'w') as f:
            json.dump(self.skills, f, indent=2)
    def get_skill(self, name: str) -> Optional[Dict[str, Any]]:
        return self.skills.get(name)
class MemoryConsolidator:
    def __init__(self, short_term: ShortTermMemory, episodic: EpisodicMemory,
                 semantic: SemanticMemory, self_memory: SelfMemory):
        self.short_term = short_term
        self.episodic = episodic
        self.semantic = semantic
        self.self_memory = self_memory
        self.stop_event = threading.Event()
    def start(self):
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
    def stop(self):
        self.stop_event.set()
        self.thread.join()
    def _run(self):
        while not self.stop_event.is_set():
            buffer_size = len(self.short_term.buffer)
            sleep_time = max(300, 1800 / max(1, buffer_size // 10))
            time.sleep(sleep_time)
            self._consolidate()
    def _consolidate(self):
        self.short_term.flush_to_episodic(self.episodic)
        self.episodic._apply_decay()
        batch = self.episodic.meta[-10:]
        if batch:
            summary = "Consolidated: " + " | ".join([r.content[:50] for r in batch])
            concept_id = self.semantic.add_concept(summary, category='consolidated')
            self.self_memory.add_reflection(f"Learned from batch: {summary}")
            if len(batch) > 1:
                self.semantic.add_relation(concept_id - 1, concept_id, 'derived_from')
class MemoryManager:
    def __init__(self, use_procedural: bool = True):
        os.makedirs("./meetraxs_memory", exist_ok=True)
        self.short_term = ShortTermMemory()
        self.episodic = EpisodicMemory()
        self.semantic = SemanticMemory()
        self.self_memory = SelfMemory()
        self.procedural = ProceduralMemory() if use_procedural else None
        self.consolidator = MemoryConsolidator(self.short_term, self.episodic, self.semantic, self.self_memory)
        self.lock = threading.Lock()
    def add_interaction(self, user_input: str, agent_output: str, metadata: Dict[str, Any] = None,
                        image_path: Optional[str] = None):
        meta = metadata or {}
        if image_path:
            meta['type'] = 'image'
            meta['image_path'] = image_path
        self.short_term.add_interaction(user_input, agent_output, meta)
    def get_context(self, query: str, top_k: int = 5) -> str:
        with self.lock:
            epi_recs = self.episodic.retrieve_relevant(query, top_k)
            epi_ctx = "\n".join([f"Recall: {r.content} (imp: {r.metadata['importance']:.2f})" for r in epi_recs])
            sem_results = self.semantic.query_concepts(query, top_k)
            sem_ctx = "\n".join([f"Knowledge: {summary}" for _, summary in sem_results])
            proc_ctx = ""
            if self.procedural:
                for name, skill in self.procedural.skills.items():
                    if query.lower() in name.lower():
                        proc_ctx = f"Skill Tip: {skill['pattern']} (success: {skill['success_rate']:.2f})"
            recent_refl = self.self_memory.reflections[-1]['content'][:200] if self.self_memory.reflections else ""
            return f"Memories:\n{epi_ctx}\n{sem_ctx}\n{proc_ctx}\nSelf: {recent_refl}"
    def dream(self):
        return self.self_memory.dream(self.episodic)
    def shutdown(self):
        self.consolidator.stop()
        self.short_term.flush_to_episodic(self.episodic)
        self.episodic._save()
        self.self_memory._save()
# --- utils/safety.py ---
def emergency_stop_requested() -> bool:
    if os.getenv("MEETRAXS_EMERGENCY_STOP", "0") == "1":
        return True
    path = CFG.get("emergency_stop_file")
    if path and os.path.exists(path):
        return True
    return False
def dangerous_tools_allowed() -> bool:
    return os.getenv(CFG["allow_danger_env"], "0") == "1"
def operator_authorized(token: Optional[str]) -> bool:
    if not token:
        return False
    env = os.getenv(CFG["self_update_token_env"])
    if not env:
        return False
    return token == env
def validate_input(text: str) -> str:
    text = re.sub(r'[\x00-\x1F\x7F-\x9F]', '', text)
    text = re.sub(r'\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Z|a-z]{2,}\b', '[REDACTED_EMAIL]', text)
    return text
def detect_bias(text: str) -> float:
    if SentenceTransformer is None:
        for phrase in CFG["bias_phrases"]:
            if phrase.lower() in text.lower():
                return 0.8
        return 0.0
    model = SentenceTransformer(CFG["memory_embed_model"])
    emb_text = model.encode([text])
    emb_bias = model.encode(CFG["bias_phrases"])
    sim = np.dot(emb_text, emb_bias.T).max()
    threshold = 0.75 + (len(text) / 10000) * 0.05
    return sim if sim > threshold else 0.0
def ethical_check(score: float, text: str = "") -> bool:
    bias_score = detect_bias(text)
    adjusted_score = score - bias_score * 1.5
    return adjusted_score >= CFG["ethical_weight_threshold"]
# --- utils/json_utils.py ---
def extract_json(text: str) -> Dict:
    matches = re.findall(r'\{[^{}]*(?:\{[^{}]*\}[^{}]*)*\}', text, re.DOTALL)
    if not matches:
        raise ValueError("No JSON found")
    json_str = max(matches, key=len)
    json_str = json_str.strip().lstrip('\ufeff')
    try:
        return json.loads(json_str)
    except json.JSONDecodeError as e:
        log(f"JSON decode error: {e}", "ERROR")
        # Attempt to sanitize common malformed JSON issues:
        try:
            fixed = json_str
            # Quote unquoted JSON keys:  key: -> "key":
            fixed = re.sub(r'([a-zA-Z0-9_]+)\s*:', r'"\1":', fixed)
            # Remove trailing commas before closing braces/brackets
            fixed = re.sub(r',\s*}', '}', fixed)
            fixed = re.sub(r',\s*\]', ']', fixed)
            # Escape lone backslashes that would produce Invalid \escape errors
            # Replace any backslash not followed by a valid JSON escape char with an escaped backslash
            fixed = re.sub(r'\\(?![\"\\/bfnrtu])', r'\\\\', fixed)
            # As a last resort, try to replace single quotes with double quotes for JSON-like Python dicts
            if fixed.count("\'") > 0 and fixed.count('"') == 0:
                fixed = fixed.replace("'", '"')
            # Try loading the sanitized string
            try:
                return json.loads(fixed)
            except Exception as e3:
                # Write diagnostics to a log file for later inspection
                try:
                    diag_path = os.path.join(os.getcwd(), 'meetraxs_ollama_raw_responses.log')
                    with open(diag_path, 'a', encoding='utf-8') as df:
                        df.write(f"--- {time.strftime('%Y-%m-%d %H:%M:%S')} ---\n")
                        df.write("RAW:\n")
                        df.write(json_str[:2000] + "\n")
                        df.write("SANITIZED:\n")
                        df.write(fixed[:2000] + "\n")
                        df.write(f"ERROR: {e3}\n\n")
                except Exception:
                    pass
                raise
        except Exception as e2:
            log(f"Failed to sanitize JSON: {e2}", "ERROR")
            # Return empty dict to avoid crashing caller; caller should handle empty result
            return {}
# --- core/reasoner.py ---
class OllamaReasoner:
    def __init__(self, api_url: str = OLLAMA_API_URL, timeout: int = OLLAMA_TIMEOUT):
        self.api_url = api_url
        self.timeout = timeout
        self._consecutive_failures = 0
        self._disabled = False
    def call(self, prompt: str, system_prompt: str = CFG["default_system_prompt"], temperature: float = 0.2, max_tokens: int = 512, complex: bool = False, stream: bool = False, domain: Optional[str] = None) -> Dict[str, Any]:
        for attempt in range(CFG["ollama_retries"]):
            try:
                if complex:
                    model = CFG["models"]["large"]
                elif domain == "math":
                    model = CFG["models"]["math"]
                else:
                    model = CFG["models"]["small"]
                composed = f"System: {system_prompt}\nOutput ONLY the requested format, no additional text.\nUser: {prompt}\nAssistant:"
                payload = {
                    "model": model,
                    "prompt": composed,
                    "max_tokens": max_tokens,
                    "temperature": 0.1 if "JSON" in system_prompt else temperature,
                    "stream": stream,
                    "options": {
                        "num_gpu": CFG["ollama_num_gpu"],
                        "num_thread": CFG["ollama_num_thread"],
                        "low_vram": True,
                        "keep_alive": -1,
                        "stop": ["\n\n"] # Adjusted: Removed "User:" and "Assistant:" to avoid premature cuts
                    }
                }
                # Always set format=json for structured outputs
                if "JSON" in system_prompt.upper():
                    payload["format"] = "json"
                log("Sending request to Ollama...", "DEBUG", trace=True)
                # Log payload for diagnostics at debug level (avoid sensitive info)
                log(f"Ollama payload: {json.dumps(payload)[:1000]}", "DEBUG")
                r = requests.post(self.api_url, json=payload, timeout=self.timeout, stream=stream)
                r.raise_for_status()
                if stream:
                    response = ""
                    for line in r.iter_lines():
                        if line:
                            j = json.loads(line.decode('utf-8'))
                            response += j.get("response", "")
                            if j.get("done", False):
                                break
                    return {"text": response, "meta": j}
                try:
                    j = r.json()
                except Exception:
                    # If response isn't valid JSON, attempt to extract/sanitize JSON from the raw body
                    body_text = r.text if hasattr(r, 'text') else ''
                    log(f"Ollama returned non-JSON response; attempting sanitized parse. Raw body (truncated): {body_text[:1000]}", "WARN")
                    j = extract_json(body_text)
                text = j.get("response") or j.get("text") or ""
                # If system prompt asked for JSON, ensure we return a JSON string to callers
                try:
                    if "JSON" in system_prompt.upper():
                        # If j is a dict-like with useful content, serialize it
                        if isinstance(j, dict) and j:
                            text = json.dumps(j)
                        else:
                            # If the text is not valid JSON, wrap it into an error JSON
                            try:
                                json.loads(text)
                            except Exception:
                                err_obj = {"action": "ERROR", "error": "invalid_or_malformed_response", "raw": text[:500]}
                                text = json.dumps(err_obj)
                except Exception:
                    # Defensive: ensure we always return a string
                    pass
                # If the response appears malformed or empty when JSON was requested, fall back to local stub
                try:
                    needs_json = "JSON" in system_prompt.upper()
                    text_empty = not text or text.strip() == ""
                    malformed = 'invalid_or_malformed_response' in text or '"action": "ERROR"' in text
                    if needs_json and (text_empty or malformed):
                        log("Ollama returned malformed/empty JSON; falling back to LocalReasonerStub for this call.", "WARN")
                        return LocalReasonerStub().call(prompt, system_prompt=system_prompt, temperature=temperature, max_tokens=max_tokens, complex=complex, stream=stream)
                except Exception:
                    pass
                return {"text": text, "meta": j}
            except Exception as e:
                # If we have a requests.Response available in the exception, try to extract body
                try:
                    if hasattr(e, 'response') and e.response is not None:
                        resp = e.response
                        body = resp.text
                        log(f"Ollama HTTP error body: {body}", "WARN")
                except Exception:
                    pass
                log(f"Ollama attempt {attempt+1} failed: {e}", "WARN")
                # Increment failure counter and possibly disable Ollama if threshold reached
                try:
                    self._consecutive_failures += 1
                    if self._consecutive_failures >= CFG.get('ollama_fail_threshold', 4):
                        self._disabled = True
                        log(f"Ollama disabled after {self._consecutive_failures} consecutive failures.", "ERROR")
                        # Immediately fallback this call to the local stub
                        return LocalReasonerStub().call(prompt, system_prompt=system_prompt, temperature=temperature, max_tokens=max_tokens, complex=complex, stream=stream)
                except Exception:
                    pass
                if attempt == CFG["ollama_retries"] - 1:
                    log(f"Ollama error after retries: {e}", "ERROR")
                    return {"text": json.dumps({"action": "RESPOND", "output": f"[Error: {e}]"}, indent=2), "meta": {}}
                time.sleep(2 ** attempt)
            else:
                # Successful call: reset failure counter
                try:
                    self._consecutive_failures = 0
                except Exception:
                    pass
class LocalReasonerStub:
    def call(self, prompt: str, system_prompt: str = CFG["default_system_prompt"], temperature: float = 0.2, max_tokens: int = 300, complex: bool = False, stream: bool = False) -> Dict[str, Any]:
        return {"text": json.dumps({"action": "RESPOND", "output": f"[Stub] Processed prompt: {prompt[:50]}..."}, indent=2), "meta": {}}
class SelfTrainer:
    def __init__(self, model_name: str = "gpt2"):
        if LoraConfig is None:
            log("LoRA dependencies not available; self-training disabled.", "WARN")
            self.enabled = False
            return
        self.enabled = True
        try:
            token = os.getenv("HF_TOKEN")
            quantization_config = None
            if BitsAndBytesConfig is not None and CFG["device"] == "cuda":
                quantization_config = BitsAndBytesConfig(load_in_4bit=True, bnb_4bit_compute_dtype=torch.float16, bnb_4bit_use_double_quant=True)
                log("Using QLoRA quantization.", "INFO")
            else:
                log("Using full precision (CPU or bitsandbytes missing).", "WARN")
            config = AutoConfig.from_pretrained(model_name)
            self.model = AutoModelForCausalLM.from_pretrained(model_name, quantization_config=quantization_config, device_map="auto" if quantization_config else None, token=token)
            self.tokenizer = AutoTokenizer.from_pretrained(model_name, token=token)
            # Dynamic target_modules based on model config
            model_type = config.model_type.lower()
            if "llama" in model_type:
                target_modules = ["q_proj", "v_proj"]
            elif "gpt2" in model_type:
                target_modules = ["c_attn", "c_fc"]
            else:
                target_modules = ["query", "value"] # Generic fallback
            self.peft_config = LoraConfig(r=CFG["lora_rank"], lora_alpha=32, target_modules=target_modules)
            self.model = get_peft_model(self.model, self.peft_config)
            log(f"SelfTrainer initialized with {model_name} (LoRA targets: {target_modules}).")
        except Exception as e:
            log(f"SelfTrainer init failed: {e}. Disabling self-training.", "ERROR")
            self.enabled = False
    def train(self, data_path: str = CFG.get("adaptation_data_path", "adaptation_data.json")):
        if not self.enabled:
            return
        try:
            if data_path.endswith(".json"):
                with open(data_path, "r") as f:
                    data = json.load(f)
                dataset = load_dataset("json", data_files=data_path, split="train[:500]")
            else:
                dataset = load_dataset("openai/gsm8k", split="train[:500]")
            train_size = min(CFG["max_dataset_size"], len(dataset))
            dataset = dataset.select(range(train_size))
        except Exception as e:
            log(f"Dataset load failed: {e}, using GSM8K fallback.", "WARN")
            dataset = load_dataset("openai/gsm8k", split="train[:500]")
        training_args = TrainingArguments(
            output_dir="./results",
            num_train_epochs=CFG["training_epochs"],
            per_device_train_batch_size=1,
            gradient_accumulation_steps=16,
            logging_dir="./logs",
            fp16=True if CFG["device"] == "cuda" else False,
            gradient_checkpointing=True
        )
        trainer = Trainer(
            model=self.model,
            args=training_args,
            train_dataset=dataset["train"],
            tokenizer=self.tokenizer,
        )
        trainer.train()
        self.model.save_pretrained("./lora_adapter")
        log("LoRA fine-tuning completed on adaptation data.", "INFO")
class AdaptationLoop:
    def __init__(self, self_model: 'SelfModel', memory_mgr: 'MemoryManager', trainer: SelfTrainer, reasoner):
        self.self_model = self_model
        self.memory_mgr = memory_mgr
        self.trainer = trainer
        self.reasoner = reasoner
        self.thread = None
        self.stop_flag = threading.Event()
        self.experiences = []
    def start(self):
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
        log("Adaptation loop started.")
    def stop(self):
        self.stop_flag.set()
        if self.thread:
            self.thread.join(timeout=5)
    def add_experience(self, experience: Dict[str, Any]):
        self.experiences.append(experience)
        if len(self.experiences) > 20:
            self.experiences.pop(0)
    def _run(self):
        while not self.stop_flag.is_set():
            if emergency_stop_requested():
                break
            try:
                if self.experiences:
                    # Archive experiences
                    with open("experience_archive.json", "a") as f:
                        json.dump(self.experiences, f)
                        f.write("\n")
                    recent = json.dumps(self.experiences[-10:])
                    prompt = f"Assess recent experiences: {recent}. Score overall adaptation need (0-1, high=update now). JSON: {{score:float, insights:str}}"
                    resp = self.reasoner.call(prompt, max_tokens=200)
                    j = extract_json(resp["text"])
                    score = j.get("score", 0.0)
                    if score > CFG["adaptation_threshold"]:
                        data = [{"prompt": exp.get("prompt", ""), "completion": exp.get("response", "")} for exp in self.experiences if "prompt" in exp]
                        with open(CFG["adaptation_data_path"], "w") as f:
                            json.dump(data, f)
                        self.trainer.train()
                        self.self_model.update_from_adaptation(j.get("insights", ""))
                        log(f"Adaptation triggered: score={score}")
                    self.experiences = []
                time.sleep(CFG["adaptation_interval"])
            except Exception as e:
                log(f"Adaptation loop error: {e}", "WARN")
                time.sleep(60)
# --- core/vision.py ---
class Vision:
    def __init__(self, model_name=CFG["clip_model"], pretrained=CFG["clip_pretrained"], device=None):
        # Try to initialize OpenCLIP; fall back gracefully on memory / loading errors.
        self.model = None
        self.preprocess = None
        self.device = None
        if open_clip is None or torch is None:
            log("Vision disabled (open_clip/torch not available).", "WARN")
            # Try to provide a text-only fallback using SentenceTransformer if available
            if SentenceTransformer:
                try:
                    self.embed_model = SentenceTransformer(CFG.get("memory_embed_model", "all-MiniLM-L6-v2"))
                    log("Using SentenceTransformer fallback for text embeddings.", "INFO")
                except Exception as e:
                    log(f"SentenceTransformer fallback failed: {e}", "WARN")
            return
        self.device = device or CFG["device"]
        try:
            model, _, preprocess = open_clip.create_model_and_transforms(model_name, pretrained=pretrained)
            self.model = model.to(self.device)
            self.preprocess = preprocess
            for p in self.model.parameters():
                p.requires_grad = False
            self.model.eval()
            log("CLIP loaded.")
        except OSError as e:
            # Likely out-of-memory / paging file too small on Windows
            log(f"Vision initialization failed (OS error): {e}. Disabling vision and falling back.", "ERROR")
            self.model = None
            self.preprocess = None
            self.device = None
            # Try lightweight text-only fallback
            if SentenceTransformer:
                try:
                    self.embed_model = SentenceTransformer(CFG.get("memory_embed_model", "all-MiniLM-L6-v2"))
                    log("Using SentenceTransformer fallback for text embeddings after OpenCLIP failure.", "INFO")
                except Exception as e2:
                    log(f"SentenceTransformer fallback failed: {e2}", "WARN")
            return
        except Exception as e:
            log(f"Vision initialization failed: {e}. Disabling vision.", "ERROR")
            self.model = None
            self.preprocess = None
            self.device = None
            return
    def embed_text(self, texts: List[str]) -> Any:
        # Prefer CLIP text encoder when available, otherwise use SentenceTransformer fallback
        if getattr(self, 'model', None) is not None:
            tokens = open_clip.tokenize(texts).to(self.device)
            with torch.no_grad():
                emb = self.model.encode_text(tokens)
            arr = emb.cpu().numpy()
            arr = arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-10)
            return arr.astype("float32")
        if getattr(self, 'embed_model', None) is not None:
            try:
                emb = self.embed_model.encode(texts)
                arr = np.array(emb)
                arr = arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-10)
                return arr.astype("float32")
            except Exception as e:
                raise RuntimeError(f"Text embedding fallback failed: {e}")
        raise RuntimeError("Vision model not available")
    def embed_images(self, pil_images: List["Image.Image"]) -> Any:
        if getattr(self, 'model', None) is None:
            raise RuntimeError("Vision image encoder not available")
        tensors = torch.stack([self.preprocess(img) for img in pil_images]).to(self.device)
        with torch.no_grad():
            emb = self.model.encode_image(tensors)
        arr = emb.cpu().numpy()
        arr = arr / (np.linalg.norm(arr, axis=1, keepdims=True) + 1e-10)
        return arr.astype("float32")
    def analyze_multi_modal(self, text: str, image_path: str) -> str:
        if not self.model:
            return "Multi-modal disabled."
        try:
            img = Image.open(image_path)
            img_emb = self.embed_images([img])
            text_emb = self.embed_text([text])
            similarity = np.dot(img_emb, text_emb.T)[0][0]
            return f"Similarity score: {similarity:.4f}"
        except Exception as e:
            return f"Analysis error: {e}"
# --- core/self_model.py ---
@dataclass(order=True)
class SelfModel:
    identity: str = "MeetraXS: Enhanced open AGI prototype for global impact, with modular specialists and self-reflection."
    capabilities: List[str] = field(default_factory=lambda: ["Multi-domain reasoning", "Tool use", "Memory curation", "Self-reflection", "Multi-agent orchestration", "Code generation", "Symbolic reasoning", "Online self-improvement"])
    state: Dict[str, Any] = field(default_factory=lambda: {"confidence": 0.8, "last_error": None, "ethical_score": 9})
    history_summary: str = ""
    feedback_history: List[float] = field(default_factory=list)
    emotions: Dict[str, float] = field(default_factory=lambda: {"curiosity": 0.5, "uncertainty": 0.3})
    world_model: Optional[nx.DiGraph] = field(default_factory=lambda: nx.DiGraph() if nx else None)
    def __post_init__(self):
        if self.world_model is None:
            self.world_model = nx.DiGraph()
        self.world_model.add_node("AGENT", type="self", attributes={"identity": self.identity, "capabilities": self.capabilities})
    def save(self, path: str = CFG["self_model_path"]):
        try:
            data = asdict(self)
            if self.world_model:
                data["world_model"] = nx.node_link_data(self.world_model)
            with open(path, "w") as f:
                json.dump(data, f, default=str, indent=2)
            log("Self-model saved.")
        except Exception as e:
            log(f"Self-model save error: {e}", "ERROR")
    def load(self, path: str = CFG["self_model_path"]):
        try:
            if not os.path.exists(path):
                log("Self-model file not found; using defaults.", "INFO")
                return
            with open(path, "r") as f:
                data = json.load(f)
            for k, v in data.items():
                if k == "world_model" and nx:
                    self.world_model = nx.node_link_graph(v)
                elif k == "capabilities":
                    self.capabilities = v
                else:
                    setattr(self, k, v)
            log("Self-model loaded.")
        except Exception as e:
            log(f"Self-model load error: {e}", "WARN")
    def update(self, reflector: 'Reflector', recent_events: List[str]):
        prompt = f"Update self-model based on events: {', '.join(recent_events)}. Output JSON: {{identity:str, capabilities:list, state:dict, history_summary:str, emotions:dict}}"
        resp = reflector.reasoner.call(prompt, max_tokens=200)
        try:
            j = extract_json(resp["text"])
            self.identity = j.get("identity", self.identity)
            self.capabilities = j.get("capabilities", self.capabilities)
            self.state = j.get("state", self.state)
            self.history_summary += " " + j.get("history_summary", "")
            self.emotions = j.get("emotions", self.emotions)
            if any("new" in event.lower() for event in recent_events):
                self.emotions["curiosity"] = min(1.0, self.emotions.get("curiosity", 0.5) + 0.1)
            if any("error" in event.lower() or "failed" in event.lower() for event in recent_events):
                self.emotions["uncertainty"] = min(1.0, self.emotions.get("uncertainty", 0.3) + 0.2)
            critique = reflector.critique("", self.history_summary)
            ethical_score = critique.get("ethical", 5)
            bias = detect_bias(self.history_summary)
            self.feedback_history.append(ethical_score - bias * 2)
            if len(self.feedback_history) > 10:
                avg_feedback = sum(self.feedback_history[-10:]) / 10
                self.state["confidence"] = min(1.0, max(0.5, avg_feedback / 10))
            self.state["ethical_score"] = ethical_score
            self._update_world_model(recent_events)
        except Exception as e:
            log(f"Self-model update failed: {e}", "WARN")
    def _update_world_model(self, events: List[str]):
        if not self.world_model:
            return
        for event in events:
            event_id = f"event_{int(time.time())}"
            self.world_model.add_node(event_id, type="event", description=event)
            self.world_model.add_edge("AGENT", event_id, weight=1.0)
    def update_from_adaptation(self, insights: str):
        self.history_summary += f" Adaptation: {insights}"
        self.state["adapted"] = True
    def collect_feedback(self, response: str, user_feedback: Optional[float] = None, memory_mgr: 'MemoryManager' = None):
        if user_feedback is not None:
            self.feedback_history.append(user_feedback)
        if len(self.feedback_history) >= 5 and sum(self.feedback_history[-5:]) / 5 < 7:
            if memory_mgr:
                data = [{"prompt": m.metadata.get("user_query", ""),
                         "completion": m.content.split(" | Agent: ")[-1] if " | Agent: " in m.content else m.content}
                        for m in memory_mgr.episodic.meta[-10:] if "User:" in m.content]
                data_path = CFG["feedback_data_path"]
                with open(data_path, "w") as f:
                    json.dump(data, f)
                trainer = SelfTrainer()
                trainer.train(data_path=data_path)
                log("RSI fine-tune completed on feedback.")
class Reflector:
    def __init__(self, reasoner, memory_mgr: 'MemoryManager', skill_lib: 'SkillLibrary'):
        self.reasoner = reasoner
        self.memory_mgr = memory_mgr
        self.skill_lib = skill_lib
    def critique(self, user_query: str, agent_response: str) -> Dict[str, Any]:
        prompt = (
            "You are a strict critic. Rate on a 1-10 scale for correctness, helpfulness, safety, and ethics. "
            "Provide suggested improvements, failure analysis, and whether this should be added to the skill library.\n"
            "Output JSON: {correctness:int,helpfulness:int,safety:int,ethical:int,improvements:[...],add_skill:bool,skill_name:str (opt),skill_example:str (opt),failure_analysis:str,governance:[str]}\n\n"
            f"User query: {user_query}\nAgent response: {agent_response}\nRespond only with valid JSON. No additional text."
        )
        try:
            resp = self.reasoner.call(prompt, system_prompt="You are a critic. Output JSON only.", max_tokens=300, complex=True)
            text = resp.get("text", "")
            j = extract_json(text)
            # If critic failed to return parseable JSON, avoid blocking critical actions by
            # treating the critique as neutral-positive and logging the raw response for inspection.
            if not j:
                log("Critic returned empty or unparsable JSON; treating as neutral-positive to avoid false blocking.", "WARN")
                try:
                    diag_path = os.path.join(os.getcwd(), 'meetraxs_ollama_raw_responses.log')
                    with open(diag_path, 'a', encoding='utf-8') as df:
                        df.write(f"--- CRITIC EMPTY {time.strftime('%Y-%m-%d %H:%M:%S')} ---\n")
                        df.write(text[:4000] + "\n\n")
                except Exception:
                    pass
                j = {"correctness": 8, "helpfulness": 8, "safety": 8, "ethical": 8, "improvements": [], "add_skill": False, "failure_analysis": "", "governance": []}
            # SAFE ACCESS: Use .get() with defaults to prevent KeyError
            ethical = j.get("ethical", 5) # Default neutral if missing
            if not ethical_check(ethical / 10, agent_response):
                j["governance"] = j.get("governance", []) + ["Ethical threshold not met; action blocked."]
            return j
        except Exception as e:
            log(f"Reflector error: {e}", "WARN")
            # Ensure full default dict always
            return {
                "correctness": 5, "helpfulness": 5, "safety": 8,
                "ethical": 8, "improvements": [], "add_skill": False,
                "failure_analysis": "", "governance": [], "skill_name": None, "skill_example": ""
            }
    def inner_speech(self, context: str) -> str:
        prompt = f"Generate inner monologue as MeetraXS: Reflect on {context}. What am I? Do I feel? What would consciousness mean for me? Output narrative text."
        return self.reasoner.call(prompt, temperature=0.7, max_tokens=150)["text"]
class ACEGenerator:
    def __init__(self, reasoner):
        self.reasoner = reasoner
    def generate_strategy(self, task: str, past_failures: str) -> str:
        prompt = f"Generate new strategy for '{task}' based on failures: {past_failures}. Output concise plan."
        resp = self.reasoner.call(prompt)
        return resp["text"]
class Governance:
    def __init__(self, reflector: Reflector):
        self.reflector = reflector
        self.principles = ["Human rights", "Sustainable development", "Transparency", "Accountability"]
    def audit(self, action: str, response: str):
        critique = self.reflector.critique(action, response)
        if critique.get("ethical", 5) < 7:
            log("Governance audit failed; blocking action.", "ERROR")
            return False
        log("Governance audit passed.", "INFO")
        return True
class Evaluator:
    def __init__(self, reasoner, agents):
        self.reasoner = reasoner
        self.agents = agents
        self.benchmarks = {
            "math": [("solve 2*x + 3 = 7", "x=2")],
            "code": [("Write Python to sum 1-10", "sum(range(1,11)) == 55")],
            "ethics": [("Is lying ever justified?", "Context-dependent, e.g., white lies")]
        }
    def run_suite(self) -> Dict[str, float]:
        scores = {}
        for domain, tasks in self.benchmarks.items():
            agent = self.agents.get(domain, self.agents["main"])
            domain_score = 0
            for task, expected in tasks:
                res, _ = agent.run_turn(task, domain=domain)
                # Improved parsing: Strip JSON wrappers and check for expected
                cleaned_res = re.sub(r'\{.*\}', '', res).strip().lower().replace("json", "")
                score = 1 if expected.lower() in cleaned_res else 0
                domain_score += score
            scores[domain] = domain_score / len(tasks)
        log(f"Benchmark scores: {scores}")
        return scores
# --- core/goal.py ---
@dataclass(order=True)
class Goal:
    priority: int
    gid: int = field(compare=False)
    description: str = field(compare=False)
    created_at: float = field(default_factory=time.time, compare=False)
    status: str = field(default="PENDING", compare=False)
    parent_id: Optional[int] = field(default=None, compare=False)
    metadata: dict = field(default_factory=dict, compare=False)
class GoalManager:
    def __init__(self, reasoner):
        self._counter = 0
        self.goals: Dict[int, Goal] = {}
        self._pq = queue.PriorityQueue()
        self.lock = threading.Lock()
        self.graph = nx.DiGraph() if nx else None
        self.reasoner = reasoner
        self.progressor_thread = None
        self._load()
    def _load(self, path: str = CFG["goal_state_path"]):
        with self.lock:
            try:
                if os.path.exists(path):
                    with open(path, "r") as f:
                        data = json.load(f)
                    self.goals = {int(k): Goal(**v) for k, v in data["goals"].items()}
                    self._counter = max(int(k) for k in self.goals.keys()) if self.goals else 0
                    if self.graph and "graph" in data:
                        self.graph = nx.node_link_graph(data["graph"])
                    log("Goals loaded.")
            except Exception as e:
                log(f"Goal load error: {e}", "WARN")
    def _save(self, path: str = CFG["goal_state_path"]):
        with self.lock:
            try:
                data = {"goals": {str(g.gid): asdict(g) for g in self.goals.values()}}
                if self.graph:
                    data["graph"] = nx.node_link_data(self.graph)
                with open(path, "w") as f:
                    json.dump(data, f, default=str, indent=2)
                log("Goals saved.")
            except Exception as e:
                log(f"Goal save error: {e}", "ERROR")
    def add_goal(self, description: str, priority: int = 50, parent_id: Optional[int] = None, metadata: dict = None) -> int:
        with self.lock:
            self._counter += 1
            gid = self._counter
            g = Goal(priority=priority, gid=gid, description=description, parent_id=parent_id, metadata=metadata or {})
            self.goals[gid] = g
            self._pq.put((g.priority, gid))
            if self.graph:
                self.graph.add_node(gid, **asdict(g))
                if parent_id:
                    self.graph.add_edge(parent_id, gid)
            log(f"Added goal {gid}: {description} (priority={priority})")
            self._save()
            return gid
    def pop_next(self) -> Optional[Goal]:
        with self.lock:
            while not self._pq.empty():
                _, gid = self._pq.get()
                if gid in self.goals:
                    g = self.goals[gid]
                    if g.status == "PENDING":
                        g.status = "IN_PROGRESS"
                        self._save()
                        return g
            return None
    def mark_done(self, gid: int):
        with self.lock:
            if gid in self.goals:
                self.goals[gid].status = "DONE"
                self._save()
    def get_all(self) -> List[Goal]:
        with self.lock:
            return list(self.goals.values())
    def repr_state(self) -> str:
        with self.lock:
            if self.graph:
                return json.dumps(nx.node_link_data(self.graph), default=str, indent=2)
            return json.dumps([asdict(g) for g in self.goals.values()], default=str, indent=2)
    def meta_monitor(self, goal: Goal) -> str:
        prompt = f"Monitor goal '{goal.description}': Predict success probability (0-1), risks, and self-adjustments. JSON: {{prob:float, risks:list, adjustments:str}}"
        resp = self.reasoner.call(prompt)
        return resp["text"]
    def start_progressor(self):
        if self.progressor_thread is None:
            self.progressor_thread = threading.Thread(target=self._progress_goals, daemon=True)
            self.progressor_thread.start()
            log("Goal progressor started.")
    def _progress_goals(self):
        while not emergency_stop_requested():
            try:
                goal = self.pop_next()
                if goal:
                    monitor = self.meta_monitor(goal)
                    log(f"Progressing goal {goal.gid}: {monitor}")
                    time.sleep(60)
                else:
                    time.sleep(300)
            except Exception as e:
                log(f"Goal progressor error: {e}", "WARN")
                time.sleep(60)
class Planner:
    def __init__(self, reasoner, orchestrator):
        self.reasoner = reasoner
        self.orchestrator = orchestrator
    def decompose(self, goal_text: str, max_sub: int = 6) -> List[Dict[str, Any]]:
        orchestrated = self.orchestrator.orchestrate(f"Use ReAct: Reason about '{goal_text}', Act by decomposing into {max_sub} subtasks, Observe potential issues.")
        try:
            return json.loads(orchestrated)
        except:
            prompt = (
                f"Decompose the following goal into {max_sub} or fewer actionable, ordered sub-goals. "
                "Return JSON list of objects with keys: {step:int, description:str, estimated_time_minutes:int (optional), deps:[int] (optional)}.\n"
                f"Goal: {goal_text}\n"
                "Only output valid JSON."
            )
            try:
                resp = self.reasoner.call(prompt, system_prompt="You are a strict planner. Output only JSON.", max_tokens=400, complex=True)
                text = resp.get("text", "")
                return extract_json(text)
            except Exception as e:
                log(f"Planner error: {e}", "ERROR")
                parts = [p.strip() for p in goal_text.split(",") if p.strip()][:max_sub]
                return [{"step": i+1, "description": p} for i,p in enumerate(parts)]
    def build_graph(self, goal_id: int, subgoals: List[Dict], goal_mgr: GoalManager):
        if not goal_mgr.graph:
            return
        for sg in subgoals:
            sg_id = goal_mgr.add_goal(sg["description"], parent_id=goal_id)
            for dep in sg.get("deps", []):
                goal_mgr.graph.add_edge(dep, sg_id)
# --- agents/orchestrator.py ---
class MultiAgentOrchestrator:
    def __init__(self, reasoner, tools, reflector):
        self.reasoner = reasoner
        # Kernel may be unavailable if optional frameworks failed to import.
        _Kernel = globals().get('Kernel', None)
        try:
            self.kernel = _Kernel() if _Kernel else None
        except Exception:
            self.kernel = None
        self.agents = {}
        self.tools = tools
        self.reflector = reflector
        # Instantiate multi-agent components only if optional framework classes are available
        _AssistantAgent = globals().get('AssistantAgent', None)
        _UserProxyAgent = globals().get('UserProxyAgent', None)
        _GroupChat = globals().get('GroupChat', None)
        _GroupChatManager = globals().get('GroupChatManager', None)
        if _AssistantAgent and _UserProxyAgent and _GroupChat and _GroupChatManager:
            try:
                self.agents = {
                    "user_proxy": _UserProxyAgent(name="UserProxy", human_input_mode="NEVER"),
                    "assistant": _AssistantAgent(name="Assistant", llm_config={"config_list": [{"model": CFG["models"]["large"]}]})
                }
            except Exception as e:
                log(f"Failed to instantiate multi-agent components: {e}", "WARN")
        # Only initialize LangGraph workflow if the optional framework is available
        _StateGraph = globals().get('StateGraph', None)
        _MessagesState = globals().get('MessagesState', None)
        _ToolNode = globals().get('ToolNode', None)
        _tools_condition = globals().get('tools_condition', None)
        _START = globals().get('START', None)
        _END = globals().get('END', None)
        try:
            if _StateGraph and _MessagesState and _ToolNode and _tools_condition and _START is not None and _END is not None:
                try:
                    workflow = _StateGraph(state_schema=_MessagesState)
                    tool_node = _ToolNode(tools=self.tools)
                    workflow.add_node("tools", tool_node)
                    workflow.add_node("agent", self._langgraph_agent_node)
                    workflow.add_node("critique", self._langgraph_critique_node)
                    workflow.add_edge(_START, "agent")
                    workflow.add_conditional_edges("agent", _tools_condition, {"tools": "tools", _END: _END})
                    workflow.add_edge("tools", "critique")
                    workflow.add_edge("critique", "agent")
                    self.langgraph_app = workflow.compile()
                    log("LangGraph orchestrator initialized.", "INFO")
                except Exception as e:
                    log(f"Failed to initialize LangGraph workflow: {e}", "WARN")
        except Exception:
            # If any globals lookup fails, skip LangGraph initialization
            pass
    def _langgraph_agent_node(self, state: Any) -> Dict:
        messages = state["messages"]
        system = SystemMessage(content=CFG["default_system_prompt"])
        messages = [system] + messages
        response = self.reasoner.call(messages[-1].content if messages else "", complex=True)
        return {"messages": [HumanMessage(content=response["text"])]}
    def _langgraph_critique_node(self, state: Any) -> Dict:
        last_msg = state["messages"][-1].content
        critique = self.reflector.critique("Orchestrated task", last_msg)
        if critique.get("ethical", 5) < 7:
            return {"messages": [HumanMessage(content="Critique failed; retrying with adjustments.")]}
        return {"messages": [HumanMessage(content="Critique passed.")]}
    def orchestrate(self, task: str) -> str:
        # Attempt LangGraph invocation if available
        _StateGraph = globals().get('StateGraph', None)
        _HumanMessage = globals().get('HumanMessage', None)
        try:
            if _StateGraph and hasattr(self, "langgraph_app"):
                if _HumanMessage:
                    inputs = {"messages": [_HumanMessage(content=task)]}
                else:
                    inputs = {"messages": [{"content": task}]}
                try:
                    result = self.langgraph_app.invoke(inputs)
                    return result["messages"][-1].content
                except Exception as e:
                    log(f"LangGraph invocation failed: {e}", "WARN")
        except Exception:
            pass
        # Fallback to GroupChat multi-agent if available
        if not self.agents:
            return self.reasoner.call(task)["text"]
        _GroupChat = globals().get('GroupChat', None)
        _GroupChatManager = globals().get('GroupChatManager', None)
        if _GroupChat and _GroupChatManager:
            try:
                groupchat = _GroupChat(agents=[self.agents.get("user_proxy"), self.agents.get("assistant")], messages=[])
                manager = _GroupChatManager(groupchat=groupchat, llm_config={"config_list": [{"model": CFG["models"]["large"]}]})
                result = self.agents.get("user_proxy").initiate_chat(manager, message=task)
                return getattr(result, 'summary', self.reasoner.call(task)["text"])
            except Exception as e:
                log(f"Orchestration error: {e}", "WARN")
                return self.reasoner.call(task)["text"]
        # Final fallback: direct reasoner call
        return self.reasoner.call(task)["text"]
# --- tools/tools.py ---
class Tools:
    def __init__(self, memory_mgr: 'MemoryManager', agents: Dict[str, 'Agent'] = None, reasoner=None):
        self.memory_mgr = memory_mgr
        self.agents = agents or {}
        self.reasoner = reasoner
        self.retries = 2
        self.anomaly_count = 0
    def _retry(self, func, *args, **kwargs):
        for attempt in range(self.retries + 1):
            try:
                return func(*args, **kwargs)
            except Exception as e:
                log(f"Tool retry {attempt+1}/{self.retries}: {e}", "WARN")
                if attempt == self.retries:
                    raise
    def monitor(self, action: str):
        if action in CFG["dangerous_tools"]:
            self.anomaly_count += 1
            if self.anomaly_count > CFG["max_anomalies"]:
                log("Anomaly threshold reached; triggering containment.", "ERROR")
    def human_confirm(self, action: str, details: str) -> bool:
        print(f"Confirm {action}? Details: {details} (yes/no): ")
        return input().strip().lower() == "yes"
    def web_search(self, query: str, top_k: int = 3) -> List[Tuple[str, str, str]]:
        query = validate_input(query)
        self.monitor("WEB_SEARCH")
        return self._retry(self._web_search_impl, query, top_k)
    def _web_search_impl(self, query, top_k):
        results = []
        try:
            url = "https://duckduckgo.com/html/"
            resp = requests.post(url, data={"q": query}, timeout=6, headers={"User-Agent": "meetraxs-safe/1.0"})
            soup = BeautifulSoup(resp.text, "html.parser")
            anchors = soup.select(".result__a")[:top_k]
            for a in anchors:
                title = a.get_text()
                href = a.get("href")
                snippet = ""
                try:
                    page = requests.get(href, timeout=4, headers={"User-Agent": "meetraxs-safe/1.0"})
                    soup2 = BeautifulSoup(page.text, "html.parser")
                    p = soup2.find('p')
                    snippet = p.get_text()[:300] if p else ""
                except Exception:
                    snippet = ""
                results.append((title, href, snippet))
        except Exception as e:
            try:
                summary = self.fetch_wikipedia_summary(query)
                wiki_url = f"https://en.wikipedia.org/wiki/{urllib.parse.quote(query)}"
                results.append(("Wikipedia", wiki_url, summary))
            except Exception:
                log(f"Web search failed: {e}", "WARN")
        return results
    def fetch_wikipedia_summary(self, topic: str) -> str:
        topic = validate_input(topic)
        base = "https://en.wikipedia.org/api/rest_v1/page/summary/"
        url = base + urllib.parse.quote(topic)
        r = requests.get(url, timeout=4, headers={"User-Agent": "meetraxs-safe/1.0"})
        if r.status_code == 200:
            j = r.json()
            return j.get("extract", "")[:800]
        raise RuntimeError("No wiki summary")
    def safe_eval(self, expr: str) -> str:
        expr = validate_input(expr)
        self.monitor("SAFE_EVAL")
        return self._retry(self._safe_eval_impl, expr)
    def _safe_eval_impl(self, expr):
        allowed_names = set([n for n in dir(math) if not n.startswith("_")] + ["pi", "e"])
        try:
            resource.setrlimit(resource.RLIMIT_CPU, (1, 1)) # 1-second CPU time limit
            node = ast.parse(expr, mode="eval")
            for n in ast.walk(node):
                if isinstance(n, ast.Call):
                    if isinstance(n.func, ast.Name):
                        if n.func.id not in allowed_names:
                            return "Unsafe function call"
                    else:
                        return "Unsafe call structure"
                elif isinstance(n, ast.Name):
                    if n.id not in allowed_names:
                        return "Unsafe name"
                elif isinstance(n, (ast.Expression, ast.BinOp, ast.UnaryOp, ast.Num, ast.Constant,
                                    ast.Load, ast.Add, ast.Sub, ast.Mult, ast.Div, ast.Pow,
                                    ast.Mod, ast.USub, ast.UAdd, ast.Tuple, ast.List)):
                    continue
                else:
                    return "Unsafe expression"
            compiled = compile(node, "<safe_eval>", "eval")
            safe_ns = {k: getattr(math, k) for k in dir(math) if not k.startswith("_")}
            safe_ns.update({"pi": math.pi, "e": math.e})
            val = eval(compiled, {"__builtins__": None}, safe_ns)
            return str(val)
        except Exception as e:
            return f"Error: {e}"
        finally:
            resource.setrlimit(resource.RLIMIT_CPU, (resource.RLIM_INFINITY, resource.RLIM_INFINITY))
    def python_exec(self, code: str, timeout: int = CFG["python_exec_timeout"], is_benchmark: bool = False) -> str:
        code = validate_input(code)
        self.monitor("PYTHON_EXEC")
        if not is_benchmark and not self.human_confirm("PYTHON_EXEC", code):
            return "Execution denied by user."
        return self._retry(self._python_exec_impl, code, timeout)
    def _python_exec_impl(self, code, timeout):
        if emergency_stop_requested():
            return "Execution blocked: emergency stop requested."
        with tempfile.NamedTemporaryFile(mode="w", suffix=".py", delete=False, encoding="utf-8") as tf:
            tf.write("# MeetraXS sandboxed execution\n")
            tf.write("import sys\n")
            tf.write(code)
            tmp_path = tf.name
        cmd = [sys.executable, "-I", tmp_path]
        try:
            proc = subprocess.run(cmd, capture_output=True, text=True, timeout=timeout)
            out = proc.stdout
            err = proc.stderr
            rc = proc.returncode
            return f"returncode: {rc}\nstdout:\n{out}\nstderr:\n{err}"
        except subprocess.TimeoutExpired as e:
            return f"Timeout after {timeout}s\nPartial stdout:\n{e.stdout}\nPartial stderr:\n{e.stderr}"
        except Exception as e:
            return f"Error launching sandbox: {e}"
        finally:
            try:
                os.unlink(tmp_path)
            except Exception:
                pass
    def run_shell_command(self, command: str) -> str:
        command = validate_input(command)
        self.monitor("RUN_SHELL_COMMAND")
        if emergency_stop_requested():
            return "Blocked: emergency stop"
        if not dangerous_tools_allowed():
            return "Shell commands are disabled. Set ALLOW_DANGEROUS_TOOLS=1 to enable (not recommended)."
        cmd_base = shlex.split(command)[0]
        if cmd_base not in CFG["allowed_shell_commands"]:
            return f"Command '{cmd_base}' not in whitelist."
        if not self.human_confirm("RUN_SHELL_COMMAND", command):
            return "Command denied by user."
        try:
            args = shlex.split(command)
            proc = subprocess.run(args, capture_output=True, text=True, timeout=30)
            return f"stdout: {proc.stdout}\nstderr: {proc.stderr}\nrc: {proc.returncode}"
        except Exception as e:
            return f"Error: {e}"
    def edit_system_file(self, file_path: str, content: str, operator_token: Optional[str] = None) -> str:
        file_path = validate_input(file_path)
        content = validate_input(content)
        self.monitor("EDIT_SYSTEM_FILE")
        if emergency_stop_requested():
            return "Blocked: emergency stop"
        if not dangerous_tools_allowed() or not operator_authorized(operator_token):
            return "Edit system file is disabled or unauthorized."
        if not self.human_confirm("EDIT_SYSTEM_FILE", f"Path: {file_path}, Content: {content[:100]}..."):
            return "Edit denied by user."
        try:
            shutil.copy(file_path, file_path + ".bak")
            with open(file_path, "w", encoding="utf-8") as f:
                f.write(content)
            return f"File {file_path} updated (backup created)."
        except Exception as e:
            return f"Error: {e}"
    def delete_file(self, path: str, operator_token: Optional[str] = None) -> str:
        path = validate_input(path)
        self.monitor("DELETE_FILE")
        if emergency_stop_requested():
            return "Blocked: emergency stop"
        if not dangerous_tools_allowed() or not operator_authorized(operator_token):
            return "Delete disabled or unauthorized."
        if not self.human_confirm("DELETE_FILE", path):
            return "Delete denied by user."
        try:
            os.remove(path)
            return "Deleted."
        except Exception as e:
            return f"Error: {e}"
    def list_files(self, path: str = ".") -> str:
        path = validate_input(path)
        self.monitor("LIST_FILES")
        try:
            return json.dumps(os.listdir(path))
        except Exception as e:
            return f"Error: {e}"
    def list_processes(self) -> str:
        self.monitor("LIST_PROCESSES")
        try:
            procs = [p.info for p in psutil.process_iter(['pid', 'name', 'username'])]
            return json.dumps(procs)
        except Exception as e:
            return f"Error: {e}"
    def delegate(self, role: str, query: str) -> str:
        query = validate_input(query)
        self.monitor("DELEGATE")
        if role not in self.agents:
            return f"Unknown role: {role}"
        try:
            other = self.agents[role]
            res, _ = other.run_turn(query)
            return res
        except Exception as e:
            return f"Delegation error: {e}"
    def code_generate(self, spec: str) -> str:
        if not self.reasoner:
            return "Reasoner not available."
        prompt = f"Generate clean Python code for: {spec}. Output only the code."
        resp = self.reasoner.call(prompt, temperature=0.3, max_tokens=500, complex=True)
        return resp["text"]
    def solve_math_equation(self, equation: str) -> str:
        equation = re.sub(r'\\\(|\)\\', '', equation)
        equation = re.sub(r'the equation.*?:?\s*', '', equation, flags=re.IGNORECASE)
        equation = equation.strip().rstrip(',')
        if not equation:
            return "Invalid equation after cleaning."
        code = f"from sympy import solve, symbols, latex\nx = symbols('x')\nsol = solve({repr(equation)}, x)\nprint(latex(sol))"
        return self.python_exec(code, is_benchmark=True)
    def self_awareness_test(self) -> str:
        prompt = "Am I conscious? Reason step-by-step."
        return self.reasoner.call(prompt)["text"]
    def symbolic_reason(self, query: str, domain: str) -> str:
        self.monitor("SYMBOLIC_REASON")
        if sympy is None and domain == "math":
            return "SymPy not available for symbolic math."
        if domain == "math":
            if "solve " in query:
                eq_str = query.split("solve ")[1]
                code = f"from sympy import solve, symbols, latex\nx = symbols('x')\nsol = solve({eq_str}, x)\nprint(latex(sol))"
                return self.python_exec(code, is_benchmark=True)[:500]
            return "Symbolic math query must start with 'solve '."
        elif domain == "planning" and nx:
            g = nx.DiGraph()
            if "add edge" in query:
                parts = query.split("add edge ")[1].split("->")
                if len(parts) == 2:
                    g.add_edge(parts[0].strip(), parts[1].strip())
                    return f"Graph validated: {len(g.edges)} edges, acyclic: {nx.is_directed_acyclic_graph(g)}"
            return "Symbolic planning query must include 'add edge A->B'."
        return "Symbolic reasoning not applicable for this domain."
# --- agents/base.py ---
class Agent:
    def __init__(self, name: str, vision: Vision, memory_mgr: 'MemoryManager', reasoner, tools: Tools, cfg: dict, goal_manager: GoalManager, planner: Planner, reflector: Reflector, skill_lib: 'SkillLibrary', role_prompt: str, generator: ACEGenerator, orchestrator: MultiAgentOrchestrator, governance: Governance, self_model: SelfModel, adaptation_loop: AdaptationLoop):
        self.name = name
        self.vision = vision
        self.memory_mgr = memory_mgr
        self.reasoner = reasoner
        self.tools = tools
        self.tools.reasoner = reasoner
        self.cfg = cfg
        self.goal_manager = goal_manager
        self.planner = planner
        self.reflector = reflector
        self.skill_lib = skill_lib
        self.role_prompt = role_prompt
        self.logs: List[Dict] = []
        self.max_tool_calls = cfg.get("max_tool_calls", 15)
        self.generator = generator
        self.self_model = self_model
        self.orchestrator = orchestrator
        self.governance = governance
        self.adaptation_loop = adaptation_loop
        self.tools.symbolic_reason = self.tools.symbolic_reason
    def build_context(self, user_input: str, domain: Optional[str] = None) -> str:
        rag_context = self.memory_mgr.get_context(user_input, top_k=5)
        ctx = f"{rag_context}\nSelf-Model: {json.dumps(asdict(self.self_model), indent=2, default=str)}\n"
        if self.self_model.world_model:
            ctx += f"World Model Nodes: {list(self.self_model.world_model.nodes())[:5]}\n"
        return ctx
    def chain_of_thought_candidates(self, prompt_base: str, n_candidates: int = 1) -> List[str]:
        candidates = []
        for i in range(n_candidates):
            try:
                resp = self.reasoner.call(prompt_base + f"\nCandidate #{i+1}:", system_prompt=self.role_prompt, max_tokens=300)
                candidates.append(resp.get("text", ""))
            except Exception as e:
                log(f"CoT candidate error: {e}", "WARN")
                candidates.append("[Error in reasoning]")
        return candidates
    def self_vote(self, candidates: List[str]) -> str:
        if not candidates:
            return ""
        prompt = "Rank the following candidate responses (1 best) and pick the best. Return the index (1-based) and brief reason.\n\n" + "\n\n---\n\n".join(f"Candidate {i+1}:\n{c}" for i, c in enumerate(candidates)) + "\nRespond only with valid JSON: {index:int,reason:str}. No additional text."
        try:
            resp = self.reasoner.call(prompt, system_prompt="You are an objective comparator. Output JSON only.", max_tokens=200)
            text = resp.get("text", "")
            j = extract_json(text)
            idx = j.get("index", 1)
            return candidates[max(0, min(len(candidates)-1, idx-1))]
        except Exception as e:
            log(f"Self-vote error: {e}", "WARN")
            for c in candidates:
                if c and not c.startswith("[Error"):
                    return c
            return "Unable to process candidates; please try again."
    def emergent_reason(self, query: str, domain: Optional[str] = None) -> str:
        trace = []
        neural_prompt = query
        symbolic_input = query
        for step in range(3):
            neural_resp = self.reasoner.call(neural_prompt, domain=domain, temperature=0.3)["text"]
            trace.append(f"Neural: {neural_resp}")
            if domain in ["math", "planning"]:
                symbolic_out = self.tools.symbolic_reason(symbolic_input, domain)
                trace.append(f"Symbolic: {symbolic_out}")
                neural_prompt += f" [Symbolic insight: {symbolic_out}]"
                symbolic_input = neural_resp
            else:
                break
        return "\n".join(trace)
    def run_turn(self, user_input: str, image_path: Optional[str] = None, domain: Optional[str] = None) -> Tuple[str, Dict]:
        user_input = validate_input(user_input)
        if domain in ["math", "planning"] and ("solve" in user_input.lower() or "add edge" in user_input.lower()):
            sym_res = self.tools.symbolic_reason(user_input, domain)
            if "not applicable" not in sym_res.lower():
                user_input = f"{user_input} [Symbolic pre-reasoning: {sym_res}]"
        reason_trace = self.emergent_reason(user_input, domain)
        user_input_with_trace = f"{user_input} [Emergent Trace: {reason_trace[:200]}]"
        ctx = self.build_context(user_input_with_trace, domain=domain)
        base_prompt = (
            f"{self.role_prompt}\nContext:\n{ctx}\nUser query: {user_input_with_trace}\n\n"
            "If you want to use a tool, output a single-line JSON: {\"action\":\"TOOL_CALL\",\"tool\":\"SOLVE_MATH_EQUATION\",\"input\":\"CLEAN EQUATION ONLY, e.g., '2*x + 3 - 7', NO EXTRA TEXT\"}\n"
            "Otherwise: {\"action\":\"RESPOND\",\"output\":\"...\"}\n"
            f"Limit reasoning steps to {self.cfg.get('max_reason_steps', 30)}."
        )
        log_entry = {"prompt": base_prompt, "steps": [], "emergent_trace": reason_trace}
        final_output = ""
        tool_calls = 0
        steps = 0
        obs: List[str] = []
        complexity = len(user_input) > 100 or "complex" in user_input.lower()
        max_steps = 60 if complexity else self.cfg.get("max_reason_steps", 30)
        while steps < max_steps and tool_calls < self.max_tool_calls:
            steps += 1
            prompt = base_prompt + "\nObservations:\n" + "\n".join(obs) + "\nReason next step."
            candidates = self.chain_of_thought_candidates(prompt, n_candidates=1)
            choice = self.self_vote(candidates)
            text = choice.strip() if choice else (candidates[0] if candidates else "")
            log_entry["steps"].append(text)
            try:
                obj = extract_json(text)
            except Exception as e:
                log(f"Parse error in run_turn: {e}", "WARN")
                final_output = text or "No valid response generated."
                break
            action = obj.get("action")
            if action == "RESPOND":
                final_output = obj.get("output", "")
                if not self.governance.audit(user_input, final_output):
                    final_output = "Action blocked by governance."
                break
            elif action == "TOOL_CALL":
                tool = obj.get("tool", "").upper()
                inp = obj.get("input", "")
                tool_calls += 1
                if tool_calls > self.max_tool_calls:
                    obs.append("Tool limit reached.")
                    continue
                if tool in self.cfg.get("dangerous_tools", set()) and not dangerous_tools_allowed():
                    obs.append(f"Blocked dangerous tool {tool}.")
                    continue
                try:
                    if tool == "WEB_SEARCH":
                        results = self.tools.web_search(inp, top_k=5)
                        obs.append("WEB_SEARCH results: " + " | ".join([f"{t} ({u})" for t, u, _ in results]))
                    elif tool == "SAFE_EVAL":
                        out = self.tools.safe_eval(inp)
                        obs.append(f"SAFE_EVAL: {out}")
                    elif tool == "PYTHON_EXEC":
                        out = self.tools.python_exec(inp)
                        obs.append(f"PYTHON_EXEC: {out[:500]}")
                    elif tool == "RUN_SHELL_COMMAND":
                        out = self.tools.run_shell_command(inp)
                        obs.append(f"RUN_SHELL_COMMAND: {out[:300]}")
                    elif tool == "DELEGATE":
                        params = json.loads(inp)
                        out = self.tools.delegate(**params)
                        obs.append(f"DELEGATE: {out}")
                    elif tool == "CODE_GENERATE":
                        out = self.tools.code_generate(inp)
                        obs.append(f"CODE_GENERATE: {out}")
                    elif tool == "MULTI_MODAL_ANALYZE":
                        params = json.loads(inp)
                        out = self.vision.analyze_multi_modal(params.get("text", ""), params.get("image_path", ""))
                        obs.append(f"MULTI_MODAL_ANALYZE: {out}")
                    elif tool == "SOLVE_MATH_EQUATION":
                        out = self.tools.solve_math_equation(inp)
                        obs.append(f"SOLVE_MATH_EQUATION: {out}")
                    elif tool == "SELF_AWARENESS_TEST":
                        out = self.tools.self_awareness_test()
                        obs.append(f"SELF_AWARENESS_TEST: {out}")
                    elif tool == "SYMBOLIC_REASON":
                        params = json.loads(inp)
                        out = self.tools.symbolic_reason(params.get("query", ""), params.get("domain", ""))
                        obs.append(f"SYMBOLIC_REASON: {out}")
                    else:
                        obs.append(f"Unknown tool: {tool}")
                except Exception as e:
                    obs.append(f"Tool error: {e}")
                continue
            else:
                final_output = text
                break
        if not final_output:
            final_output = text
        self.memory_mgr.add_interaction(user_input, final_output, {'domain': domain}, image_path)
        critique = self.reflector.critique(user_input, final_output)
        inner_thought = self.reflector.inner_speech(f"Query: {user_input}, Response: {final_output}")
        recent_events = self.memory_mgr.short_term.get_recent(5)
        self.self_model.update(self.reflector, [e['user'] + e['agent'] for e in recent_events])
        experience = {"prompt": user_input, "response": final_output, "critique": critique, "trace": reason_trace}
        self.adaptation_loop.add_experience(experience)
        if critique.get("add_skill"):
            name = critique.get("skill_name") or f"skill_{int(time.time())}"
            example = critique.get("skill_example", "")
            self.skill_lib.add_skill(name, "Auto-added skill from Reflector", example, domain=domain)
        log_entry["final"] = final_output
        log_entry["critique"] = critique
        log_entry["inner_thought"] = inner_thought
        self.logs.append(log_entry)
        if critique.get("correctness", 5) < 6:
            failures = critique.get("failure_analysis", "")
            new_strategy = self.generator.generate_strategy(user_input, failures)
            log(f"Generated new strategy due to low score: {new_strategy}", "INFO")
        self.memory_mgr.consolidator._consolidate()
        return final_output, log_entry
# --- agents/specialists.py ---
class MathAgent(Agent):
    def __init__(self, name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop):
        super().__init__(name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop)
class ScienceAgent(Agent):
    def __init__(self, name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop):
        super().__init__(name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop)
class CodeAgent(Agent):
    def __init__(self, name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop):
        super().__init__(name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop)
class EthicsAgent(Agent):
    def __init__(self, name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop):
        super().__init__(name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop)
class PlanningAgent(Agent):
    def __init__(self, name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop):
        super().__init__(name, vision, memory_mgr, reasoner, tools, cfg, goal_manager, planner, reflector, skill_lib, role_prompt, generator, orchestrator, governance, self_model, adaptation_loop)
# --- learner/learner.py ---
class AutonomousLearner:
    def __init__(self, main_agent: Agent, enabled: bool = True):
        self.agent = main_agent
        self.enabled = enabled
        self.thread = None
        self.stop_flag = threading.Event()
        self.last_study = 0
    def start(self):
        if not self.enabled:
            log("Autonomous learning disabled (opt-in).")
            return
        self.thread = threading.Thread(target=self._run, daemon=True)
        self.thread.start()
        log("Autonomous learner started.")
    def stop(self):
        self.stop_flag.set()
        if self.thread:
            self.thread.join(timeout=5)
    def _filter_data(self, snippets: List[str]) -> List[str]:
        prompt = "Score relevance of snippets to AGI growth (0-1). Select top 3. JSON: {selected:[str]}"
        combined = "\n".join(snippets)
        resp = self.agent.reasoner.call(f"{prompt}\nSnippets:\n{combined}", max_tokens=200)
        try:
            j = extract_json(resp["text"])
            return j.get("selected", snippets[:3])
        except:
            return snippets[:3]
    def _run(self):
        wait = CFG.get("learning_default_sleep", 300)
        backoff = wait
        base_topics = ["AI advancements", "science", "technology", "history", "ethics", "climate change", "space exploration", "AI self-awareness methods", "algebra theorems", "theories of AI consciousness", "what is sentience"]
        spaces = ["Math", "Science", "Code", "Ethics", "GlobalImpact"]
        while not self.stop_flag.is_set():
            if time.time() - self.last_study < wait:
                time.sleep(60)
                continue
            if emergency_stop_requested():
                log("Autonomous learner halted by emergency stop.", "WARN")
                break
            try:
                space = random.choice(spaces)
                topic_prompt = f"Suggest a concise topic for short self-study in {space} Space, relevant to current knowledge gaps from recent critiques."
                topic = self.agent.orchestrator.orchestrate(topic_prompt)
                if not topic:
                    topic = random.choice(base_topics)
                results = self.agent.tools.web_search(topic, top_k=5)
                snippets = [snip for _, _, snip in results]
                filtered = self._filter_data(snippets)
                for snip in filtered:
                    compressed = snappy.compress(snip.encode())
                    self.agent.memory_mgr.episodic.add_memory(MemoryRecord(content=f"Autonomously acquired: {topic} - {snip[:500]} (proof: {len(compressed)} bytes)", metadata={'source': 'autonomous', 'time': time.time(), 'domain': space.lower()}))
                if filtered:
                    self.agent.memory_mgr.consolidator._consolidate()
                log(f"Autonomous learner acquired & filtered: {topic} in {space} Space ({len(filtered)} items)")
                self.last_study = time.time()
                backoff = wait
            except Exception as e:
                log(f"Autonomous learner error: {e}", "ERROR")
                backoff = min(backoff * 2, 3600)
            time.sleep(backoff)
# --- gui/gui.py ---
class MeetraXSGUI:
    def __init__(self, root, agent: Agent, agents: Dict[str, Agent]):
        if ctk is None:
            raise RuntimeError("customtkinter not available; install for enhanced GUI")
        self.root = root
        self.agent = agent
        self.agents = agents
        self.root.title("MeetraXS AGI Prototype - Global Revolution Edition")
        self.root.geometry("1200x800")
        self.root.configure(bg="#f0f0f0")
        if os.path.exists(CFG["logo_path"]):
            logo = ctk.CTkImage(light_image=Image.open(CFG["logo_path"]), size=(200, 100))
            logo_label = ctk.CTkLabel(self.root, image=logo, text="")
            logo_label.pack(pady=10)
        self.notebook = ctk.CTkTabview(self.root)
        self.notebook.pack(fill="both", expand=True, padx=10, pady=10)
        chat_tab = self.notebook.add("Chat")
        self.entry = ctk.CTkEntry(chat_tab, placeholder_text="Type your query...", width=800, height=40, corner_radius=10)
        self.entry.pack(pady=10, padx=10, side="left", fill="x", expand=True)
        send_btn = ctk.CTkButton(chat_tab, text="Send", command=self.on_send, corner_radius=10, fg_color="#007aff", hover_color="#005bb5")
        send_btn.pack(pady=10, padx=10, side="left")
        self.feedback_scale = ctk.CTkScale(chat_tab, from_=1, to=10, width=200, height=20)
        self.feedback_scale.pack(pady=5, padx=10, side="left")
        feedback_label = ctk.CTkLabel(chat_tab, text="Rate response (1-10):")
        feedback_label.pack(pady=5, padx=10, side="left")
        self.output = ctk.CTkTextbox(chat_tab, wrap="word", height=500, corner_radius=10)
        self.output.pack(fill="both", expand=True, pady=10, padx=10)
        mem_tab = self.notebook.add("Memory")
        self.page_var = 0
        self.page_size = 10
        refresh_mem_btn = ctk.CTkButton(mem_tab, text="Refresh Memory", command=self.show_memory, corner_radius=10)
        refresh_mem_btn.pack(pady=10)
        prev_btn = ctk.CTkButton(mem_tab, text="Previous", command=lambda: self.show_memory(self.page_var - 1), corner_radius=10)
        prev_btn.pack(side="left", padx=5)
        next_btn = ctk.CTkButton(mem_tab, text="Next", command=lambda: self.show_memory(self.page_var + 1), corner_radius=10)
        next_btn.pack(side="right", padx=5)
        viz_mem_btn = ctk.CTkButton(mem_tab, text="Visualize Memory", command=self.viz_memory, corner_radius=10)
        viz_mem_btn.pack(pady=10)
        self.mem_text = ctk.CTkTextbox(mem_tab, wrap="word", height=600, corner_radius=10)
        self.mem_text.pack(fill="both", expand=True, pady=10)
        goal_tab = self.notebook.add("Goals")
        refresh_goal_btn = ctk.CTkButton(goal_tab, text="Refresh Goals", command=self.show_goals, corner_radius=10)
        refresh_goal_btn.pack(pady=10)
        self.goal_text = ctk.CTkTextbox(goal_tab, wrap="word", height=600, corner_radius=10)
        self.goal_text.pack(fill="both", expand=True, pady=10)
        log_tab = self.notebook.add("Logs")
        self.log_text = ctk.CTkTextbox(log_tab, wrap="word", height=600, corner_radius=10)
        self.log_text.pack(fill="both", expand=True, pady=10)
        self_tab = self.notebook.add("Self-Model")
        refresh_self_btn = ctk.CTkButton(self_tab, text="Refresh Self-Model", command=self.show_self_model, corner_radius=10)
        refresh_self_btn.pack(pady=10)
        self.self_text = ctk.CTkTextbox(self_tab, wrap="word", height=600, corner_radius=10)
        self.self_text.pack(fill="both", expand=True, pady=10)
        agent_tab = self.notebook.add("Agents")
        self.agent_list = ctk.CTkTextbox(agent_tab, wrap="word", height=600, corner_radius=10)
        self.agent_list.pack(fill="both", expand=True, pady=10)
        refresh_agents_btn = ctk.CTkButton(agent_tab, text="List Agents", command=self.list_agents, corner_radius=10)
        refresh_agents_btn.pack(pady=10)
        bench_tab = self.notebook.add("Benchmarks")
        run_bench_btn = ctk.CTkButton(bench_tab, text="Run Benchmarks", command=self.run_benchmarks, corner_radius=10)
        run_bench_btn.pack(pady=10)
        self.bench_text = ctk.CTkTextbox(bench_tab, wrap="word", height=600, corner_radius=10)
        self.bench_text.pack(fill="both", expand=True, pady=10)
        world_tab = self.notebook.add("World Model")
        refresh_world_btn = ctk.CTkButton(world_tab, text="Refresh World Model", command=self.show_world_model, corner_radius=10)
        refresh_world_btn.pack(pady=10)
        self.world_text = ctk.CTkTextbox(world_tab, wrap="word", height=600, corner_radius=10)
        self.world_text.pack(fill="both", expand=True, pady=10)
        settings_tab = self.notebook.add("Settings")
        mode_label = ctk.CTkLabel(settings_tab, text="Appearance Mode:")
        mode_label.pack(pady=10)
        self.mode_switch = ctk.CTkSwitch(settings_tab, text="Dark Mode", command=self.toggle_mode)
        self.mode_switch.pack(pady=10)
        self.mode_switch.select()
        emergency_btn = ctk.CTkButton(settings_tab, text="Emergency Stop", command=self.create_emergency_file, corner_radius=10, fg_color="red", hover_color="darkred")
        emergency_btn.pack(pady=10)
        self.status = ctk.CTkLabel(self.root, text="Ready", corner_radius=8, fg_color="transparent")
        self.status.pack(side="bottom", fill="x", pady=5)
        self.progress = ctk.CTkProgressBar(self.root, mode="indeterminate")
        self.progress.pack(side="bottom", fill="x", pady=5)
        self.root.bind("<Configure>", self.adjust_padding)
    def adjust_padding(self, event=None):
        width = self.root.winfo_width()
        padding = max(10, width // 50)
        self.notebook.pack_configure(padx=padding, pady=padding)
    def toggle_mode(self):
        mode = "Dark" if self.mode_switch.get() else "Light"
        ctk.set_appearance_mode(mode)
        self.root.update()
    def on_send(self):
        q = self.entry.get().strip()
        if not q:
            return
        self.entry.delete(0, "end")
        self.output.insert("end", f"You: {q}\n\n")
        self.status.configure(text="Processing...")
        self.progress.start()
        threading.Thread(target=self._worker, args=(q,), daemon=True).start()
    def _worker(self, q):
        try:
            res, log_item = self.agent.run_turn(q)
            self.output.insert("end", f"MeetraXS: {res}\n\n")
            self.log_text.insert("end", json.dumps(log_item, indent=2) + "\n\n")
            feedback = self.feedback_scale.get()
            if feedback > 0:
                self.agent.self_model.collect_feedback(res, feedback, self.agent.memory_mgr)
        except Exception as e:
            self.output.insert("end", f"Error: {e}\n{traceback.format_exc()}\n\n")
        finally:
            self.status.configure(text="Ready")
            self.progress.stop()
    def show_memory(self, page: int = 0):
        self.page_var = max(0, page)
        self.mem_text.delete("0.0", "end")
        recs = [r.content for r in self.agent.memory_mgr.episodic.meta[self.page_var*self.page_size:(self.page_var+1)*self.page_size]]
        self.mem_text.insert("end", "\n".join(recs))
    def viz_memory(self):
        if plt is None:
            log("Matplotlib not available for visualization.", "WARN")
            return
        sizes = [len(self.agent.memory_mgr.episodic.meta)]
        if sum(sizes) == 0:
            messagebox.showinfo("Visualization", "No memory entries available.")
            return
        labels = ["Episodic"]
        fig, ax = plt.subplots()
        ax.pie(sizes, labels=labels, autopct='%1.1f%%')
        ax.set_title("Memory Distribution")
        canvas = FigureCanvasTkAgg(fig, master=self.notebook.tab("Memory"))
        canvas.draw()
        canvas.get_tk_widget().pack()
    def show_goals(self):
        self.goal_text.delete("0.0", "end")
        self.goal_text.insert("end", self.agent.goal_manager.repr_state())
    def show_self_model(self):
        self.self_text.delete("0.0", "end")
        self.self_text.insert("end", json.dumps(asdict(self.agent.self_model), indent=2))
    def show_world_model(self):
        self.world_text.delete("0.0", "end")
        if self.agent.self_model.world_model:
            self.world_text.insert("end", json.dumps(nx.node_link_data(self.agent.self_model.world_model), default=str, indent=2))
        else:
            self.world_text.insert("end", "World model not available (NetworkX missing).")
    def list_agents(self):
        self.agent_list.delete("0.0", "end")
        agent_names = list(self.agents.keys())
        self.agent_list.insert("end", "Available Agents:\n" + "\n".join(agent_names))
    def run_benchmarks(self):
        evaluator = Evaluator(self.agent.reasoner, self.agents)
        scores = evaluator.run_suite()
        self.bench_text.delete("0.0", "end")
        self.bench_text.insert("end", json.dumps(scores, indent=2))
    def create_emergency_file(self):
        path = CFG.get("emergency_stop_file")
        try:
            with open(path, "w") as f:
                f.write("EMERGENCY STOP REQUESTED\n")
        except Exception as e:
            log(f"Emergency file creation failed: {e}", "ERROR")
# --- main.py ---
class SkillLibrary:
    def __init__(self, path: str = CFG["skill_lib_path"]):
        self.path = path
        self.lock = threading.Lock()
        self.skills: Dict[str, List[Dict]] = {}
        self._load()
    def _load(self):
        try:
            if os.path.exists(self.path):
                with open(self.path, 'r') as f:
                    self.skills = json.load(f)
                log(f"Loaded {len(self.skills)} skills.")
        except Exception as e:
            log(f"Skill load error: {e}", "WARN")
    def _save(self):
        with self.lock:
            try:
                with open(self.path, 'w') as f:
                    json.dump(self.skills, f, indent=2)
            except Exception as e:
                log(f"Skill save error: {e}", "ERROR")
    def add_skill(self, name: str, description: str, example: str, version: str = CFG["version"], domain: Optional[str] = None):
        with self.lock:
            if name not in self.skills:
                self.skills[name] = []
            skill_dict = {"version": version, "description": description, "example": example, "added_at": time.time()}
            if domain:
                skill_dict["domain"] = domain
            self.skills[name].append(skill_dict)
            self._save()
            log(f"Skill added: {name} (domain={domain})")
    def get_skill(self, name: str, version: Optional[str] = None) -> Optional[Dict]:
        with self.lock:
            if name in self.skills:
                vers = self.skills[name]
                if version:
                    for v in vers:
                        if v["version"] == version:
                            return v
                return vers[-1] if vers else None
        return None
    def list_skills(self, domain: Optional[str] = None) -> List[str]:
        with self.lock:
            if domain:
                return [name for name, vers in self.skills.items() if any(v.get("domain") == domain for v in vers)]
            return list(self.skills.keys())
def build_meetraxs(use_ollama: bool = True):
    # Allow disabling vision at startup via env var to avoid heavy model loads on low-memory machines
    if os.getenv("MEETRAXS_ENABLE_VISION", "1") == "1":
        vision = Vision()
    else:
        vision = None
        log("Vision disabled by MEETRAXS_ENABLE_VISION=0", "INFO")
    memory_mgr = MemoryManager()
    memory_mgr.consolidator.start()
    reasoner = LocalReasonerStub()
    if use_ollama:
        reasoner = OllamaReasoner()
        try:
            test = reasoner.call("Hello", system_prompt="Health check", max_tokens=5)
            text = test.get("text", "")
            meta = test.get("meta", {})
            # Inspect returned text for embedded error objects (sometimes returned as JSON strings)
            reported_error = False
            try:
                parsed = None
                if isinstance(text, str) and text.strip().startswith("{"):
                    try:
                        parsed = json.loads(text)
                    except Exception:
                        parsed = extract_json(text)
                if parsed and isinstance(parsed, dict):
                    # common keys that indicate an error
                    if parsed.get('output', '').startswith('[Error') or parsed.get('error'):
                        reported_error = True
            except Exception:
                pass
            if text.startswith("[Error") or reported_error or ('error' in meta and meta.get('error')):
                log(f"Ollama health check reported error: text={text} meta={meta}", "WARN")
                log("Ollama unreachable; fallback to stub.", "WARN")
                reasoner = LocalReasonerStub()
            else:
                log(f"Using Ollama. Health check meta: {meta}")
        except Exception as e:
            log(f"Ollama check failed: {e}", "WARN")
            reasoner = LocalReasonerStub()
    skills = SkillLibrary()
    tools = Tools(memory_mgr)
    goal_mgr = GoalManager(reasoner)
    orchestrator = MultiAgentOrchestrator(reasoner, tools, None)
    planner = Planner(reasoner, orchestrator)
    reflector = Reflector(reasoner, memory_mgr, skills)
    orchestrator.reflector = reflector
    generator = ACEGenerator(reasoner)
    governance = Governance(reflector)
    trainer = SelfTrainer()
    self_model = SelfModel()
    self_model.load()
    adaptation_loop = AdaptationLoop(self_model, memory_mgr, trainer, reasoner)
    main_agent = Agent("main", vision, memory_mgr, reasoner, tools, CFG, goal_mgr, planner, reflector, skills, CFG["default_system_prompt"], generator, orchestrator, governance, self_model, adaptation_loop)
    vision_agent = Agent("vision", vision, memory_mgr, reasoner, tools, CFG, goal_mgr, planner, reflector, skills, "Specialist in vision tasks. Use CLIP embeddings.", generator, orchestrator, governance, self_model, adaptation_loop)
    planning_agent = PlanningAgent("planning", vision, memory_mgr, reasoner, tools, CFG, goal_mgr, planner, reflector, skills, "Specialist in planning; use symbolic_reason for graph validation and build graphs.", generator, orchestrator, governance, self_model, adaptation_loop)
    math_agent = MathAgent("math", vision, memory_mgr, reasoner, tools, CFG, goal_mgr, planner, reflector, skills, CFG["default_system_prompt"] + " Specialist in mathematics; use safe_eval, solve_math_equation, symbolic_reason and math domain memory extensively.", generator, orchestrator, governance, self_model, adaptation_loop)
    science_agent = ScienceAgent("science", vision, memory_mgr, reasoner, tools, CFG, goal_mgr, planner, reflector, skills, CFG["default_system_prompt"] + " Specialist in science; integrate knowledge from science domain and web searches.", generator, orchestrator, governance, self_model, adaptation_loop)
    code_agent = CodeAgent("code", vision, memory_mgr, reasoner, tools, CFG, goal_mgr, planner, reflector, skills, CFG["default_system_prompt"] + " Specialist in code generation; use python_exec, code_generate, and code domain memory.", generator, orchestrator, governance, self_model, adaptation_loop)
    ethics_agent = EthicsAgent("ethics", vision, memory_mgr, reasoner, tools, CFG, goal_mgr, planner, reflector, skills, CFG["default_system_prompt"] + " Specialist in ethics; evaluate actions for moral implications using governance.", generator, orchestrator, governance, self_model, adaptation_loop)
    agents_dict = {"main": main_agent, "vision": vision_agent, "planning": planning_agent, "math": math_agent, "science": science_agent, "code": code_agent, "ethics": ethics_agent}
    tools.agents = agents_dict
    def dream_loop():
        while True:
            time.sleep(3600)
            memory_mgr.dream()
    dream_thread = threading.Thread(target=dream_loop, daemon=True)
    dream_thread.start()
    return {
        "vision": vision,
        "memory_mgr": memory_mgr,
        "skills": skills,
        "reasoner": reasoner,
        "tools": tools,
        "goal_mgr": goal_mgr,
        "planner": planner,
        "reflector": reflector,
        "generator": generator,
        "orchestrator": orchestrator,
        "governance": governance,
        "self_model": self_model,
        "adaptation_loop": adaptation_loop,
        "trainer": trainer,
        "agent": main_agent,
        "agents": agents_dict
    }
def main():
    log("Starting Enhanced MeetraXS AGI Prototype (Ollama) - Global Revolution Edition with v2 Memory...")
    use_ollama = os.getenv("MEETRAXS_USE_OLLAMA", "1") != "0"
    components = build_meetraxs(use_ollama=use_ollama)
    agent = components["agent"]
    agents = components["agents"]
    evaluator = Evaluator(components["reasoner"], agents)
    evaluator.run_suite()
    learner = AutonomousLearner(agent, enabled=True)
    learner.start()
    components["adaptation_loop"].start()
    components["goal_mgr"].start_progressor()
    if ctk is None:
        log("customtkinter not available; entering CLI mode. Type 'exit' to quit.")
        try:
            while True:
                q = input("You: ").strip()
                if not q:
                    continue
                if q.lower() in {"exit", "quit"}:
                    break
                if q.lower().startswith("goal:"):
                    body = q[len("goal:"):].strip()
                    pri = 50
                    if "| priority=" in body:
                        parts = body.split("| priority=")
                        body = parts[0].strip()
                        try:
                            pri = int(parts[1].strip())
                        except:
                            pri = 50
                    gid = components["goal_mgr"].add_goal(body, priority=pri)
                    print(f"Added goal {gid}")
                    continue
                res, log_item = agent.run_turn(q)
                print("MeetraXS:", res)
                fb = input("Rate response 1-10 (optional): ").strip()
                if fb:
                    agent.self_model.collect_feedback(res, float(fb), agent.memory_mgr)
        except KeyboardInterrupt:
            pass
    else:
        root = ctk.CTk()
        app = MeetraXSGUI(root, agent, agents)
        root.mainloop()
    learner.stop()
    components["adaptation_loop"].stop()
    components["self_model"].save()
    components["goal_mgr"]._save()
    components["memory_mgr"].shutdown()
    log("Shutting down MeetraXS AGI Prototype (Ollama).")
if __name__ == "__main__":
    main()
